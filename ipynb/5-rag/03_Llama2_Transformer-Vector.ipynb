{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Llama2 Transformers Text-Vector Convert\n",
    "* HuggingFace Transformers 을 사용하여 Text를 Vector 로 변환\n",
    "* 문장 생성을 위한 transformer 객체 : LlamaForCausalLM\n",
    "* 모델이름: casllm-base-7b-hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "model_id = \"/data/aibiseo/models/casllm-base-7b-hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModel.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vector 변환\n",
    "    1. 문장을 토큰화하여 input_ids 생성\n",
    "    2. 모델을 통해 input_ids를 입력하여 임베딩 얻기\n",
    "    3. Outputs는 모델의 마지막 층에서의 출력을 포함하며, 여기에서 벡터 추출\n",
    "    4. 문장 임베딩은 출력의 평균을 위한다.\n",
    "    5. 벡터DB 에 저장할 숫자 리스트를 얻는다. vector_values_list\n",
    "        * 벡터 차원 확인하기\n",
    "        * vector_dimension = len(vector_values_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Hey, are you conscious? Can you talk to me?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embedding = outputs.last_hidden_state.mean(dim=1)\n",
    "print(sentence_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU 상의 텐서인 경우, 먼저 CPU로 이동\n",
    "vector_values = sentence_embedding.detach().cpu().numpy()\n",
    "print(vector_values)\n",
    "\n",
    "# sentence_embedding에서 벡터 값만 추출\n",
    "# 필요한 경우, NumPy 배열을 Python 리스트로 변환\n",
    "vector_values_list = vector_values.tolist()\n",
    "print(vector_values_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 벡터 차원 구하기\n",
    "vector_dimension = len(vector_values_list)\n",
    "\n",
    "print(f\"벡터 차원: {vector_dimension}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모든 작업 완료 : 100%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "llama"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

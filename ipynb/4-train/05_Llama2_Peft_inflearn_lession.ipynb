{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Llama2 Fine tuning by inflearn.com \n",
    "```\n",
    "1. excute: https://www.inflearn.com/course/lecture?courseSlug=%EB%8C%80%EA%B7%9C%EB%AA%A8-%EC%96%B8%EC%96%B4%EB%AA%A8%EB%8D%B8-llm-part1&unitId=178421\n",
    "2. model: Llama2 pytouch.bin\n",
    "3. parameter data type: touch.float16\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### requiment modules\n",
    "```\n",
    "pip install autotrain-advanced\n",
    "pip install transformers\n",
    "pip install bitsandbytes\n",
    "pip install accelerate\n",
    "pip install sentencepiece\n",
    "pip install protobuf\n",
    "pip install xformers\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autotrain 라이브러리 설치\n",
    "%pip install autotrain-advanced -q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autotrain 라이브러리 업데이트\n",
    "!autotrain setup --update-torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습을 위한 환경 변수 설정\n",
    "import os\n",
    "os.environ[\"PROJECT_NAME\"] = \"kocasalpha-sllm7b\"\n",
    "os.environ[\"MODEL_NAME\"] = \"/data/aibiseo/models/casllm-base-7b-hf\"\n",
    "os.environ[\"DATA_PATH\"] = \"/data/aibiseo/dataset/alpaca_en_dataset.jsonl\"\n",
    "os.environ[\"TEXT_COLUMN\"] = \"text\"\n",
    "os.environ[\"QUANTIZATION\"] = \"int4\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: autotrain <command> [<args>]\n",
      "AutoTrain advanced CLI: error: unrecognized arguments: --local-data-path /data/aibiseo/dataset/alpaca_en_dataset.jsonl\n"
     ]
    }
   ],
   "source": [
    "!autotrain llm --train \\\n",
    "    --project-name \"kocasalpha-sllm7b\" \\\n",
    "    --model \"/data/aibiseo/models/casllm-base-7b-hf\" \\\n",
    "    --data-path \"/data/aibiseo/dataset/alpaca_en_dataset.jsonl\" \\\n",
    "    --text_column \"text\" \\\n",
    "    --peft\\\n",
    "    --quantization \"int4\"\\\n",
    "    --lr 2e-4 \\\n",
    "    --auto-find-batch-size \\\n",
    "    --epochs 1 \\\n",
    "    --trainer sft \\\n",
    "    --block_size 4096 \\\n",
    "    --model_max_length 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[1mINFO    Running LLM\u001b[0m\n",
      "> \u001b[1mINFO    Params: Namespace(version=False, text_column='text', rejected_text_column='rejected', prompt_text_column='prompt', model_ref=None, warmup_ratio=0.1, optimizer='adamw_torch', scheduler='linear', weight_decay=0.0, max_grad_norm=1.0, add_eos_token=False, block_size=4096, peft=True, lora_r=16, lora_alpha=32, lora_dropout=0.05, logging_steps=-1, evaluation_strategy='epoch', save_total_limit=1, save_strategy='epoch', auto_find_batch_size=True, mixed_precision=None, quantization='int4', model_max_length=4096, trainer='sft', target_modules=None, merge_adapter=False, use_flash_attention_2=False, dpo_beta=0.1, chat_template=None, padding=None, train=True, deploy=False, inference=False, username=None, backend='local-cli', token=None, repo_id=None, push_to_hub=False, model='/data/aibiseo/models/casllm-base-7b-hf', project_name='kocasalpha-sllm7b', seed=42, epochs=1, gradient_accumulation=1, disable_gradient_checkpointing=False, lr=0.0002, log='none', data_path='/data/aibiseo/dataset/alpaca_en_dataset.jsonl', train_split='train', valid_split=None, batch_size=2, func=<function run_llm_command_factory at 0x7f726b291120>)\u001b[0m\n",
      "> \u001b[1mINFO    Starting local training...\u001b[0m\n",
      "> \u001b[1mINFO    {\"model\":\"/data/aibiseo/models/casllm-base-7b-hf\",\"project_name\":\"kocasalpha-sllm7b\",\"data_path\":\"/data/aibiseo/dataset/alpaca_en_dataset.jsonl\",\"train_split\":\"train\",\"valid_split\":null,\"add_eos_token\":false,\"block_size\":4096,\"model_max_length\":4096,\"padding\":null,\"trainer\":\"sft\",\"use_flash_attention_2\":false,\"log\":\"none\",\"disable_gradient_checkpointing\":false,\"logging_steps\":-1,\"evaluation_strategy\":\"epoch\",\"save_total_limit\":1,\"save_strategy\":\"epoch\",\"auto_find_batch_size\":true,\"mixed_precision\":null,\"lr\":0.0002,\"epochs\":1,\"batch_size\":2,\"warmup_ratio\":0.1,\"gradient_accumulation\":1,\"optimizer\":\"adamw_torch\",\"scheduler\":\"linear\",\"weight_decay\":0.0,\"max_grad_norm\":1.0,\"seed\":42,\"chat_template\":null,\"quantization\":\"int4\",\"target_modules\":null,\"merge_adapter\":false,\"peft\":true,\"lora_r\":16,\"lora_alpha\":32,\"lora_dropout\":0.05,\"model_ref\":null,\"dpo_beta\":0.1,\"prompt_text_column\":\"prompt\",\"text_column\":\"text\",\"rejected_text_column\":\"rejected\",\"push_to_hub\":false,\"repo_id\":null,\"username\":null,\"token\":null}\u001b[0m\n",
      "> \u001b[1mINFO    ['accelerate', 'launch', '--multi_gpu', '--num_machines', '1', '--num_processes', '2', '--mixed_precision', 'no', '-m', 'autotrain.trainers.clm', '--training_config', 'kocasalpha-sllm7b/training_params.json']\u001b[0m\n",
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "/data/pvenv/train/lib/python3.10/site-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/data/pvenv/train/lib/python3.10/site-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "[W CUDAAllocatorConfig.h:30] Warning: expandable_segments not supported on this platform (function operator())\n",
      "\u001b[31m\u001b[1m❌ ERROR \u001b[0m | \u001b[32m2024-03-13 18:38:07\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mwrapper\u001b[0m:\u001b[36m91\u001b[0m - \u001b[31m\u001b[1mtrain has failed due to an exception: Traceback (most recent call last):\n",
      "  File \"/data/pvenv/train/lib/python3.10/site-packages/autotrain/trainers/common.py\", line 88, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/data/pvenv/train/lib/python3.10/site-packages/autotrain/trainers/clm/__main__.py\", line 128, in train\n",
      "    train_data, valid_data = process_input_data(config)\n",
      "  File \"/data/pvenv/train/lib/python3.10/site-packages/autotrain/trainers/clm/__main__.py\", line 71, in process_input_data\n",
      "    train_data = load_dataset(\n",
      "  File \"/data/pvenv/train/lib/python3.10/site-packages/datasets/load.py\", line 2112, in load_dataset\n",
      "    raise ValueError(\n",
      "ValueError: You are trying to load a dataset that was saved using `save_to_disk`. Please use `load_from_disk` instead.\n",
      "\u001b[0m\n",
      "\u001b[31m\u001b[1m❌ ERROR \u001b[0m | \u001b[32m2024-03-13 18:38:09\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mwrapper\u001b[0m:\u001b[36m92\u001b[0m - \u001b[31m\u001b[1mYou are trying to load a dataset that was saved using `save_to_disk`. Please use `load_from_disk` instead.\u001b[0m\n",
      "[W CUDAAllocatorConfig.h:30] Warning: expandable_segments not supported on this platform (function operator())\n"
     ]
    }
   ],
   "source": [
    "!autotrain llm --train \\\n",
    "    --project-name \"kocasalpha-sllm7b\" \\\n",
    "    --model \"/data/aibiseo/models/casllm-base-7b-hf\" \\\n",
    "    --data-path \"/data/aibiseo/dataset/alpaca_en_dataset.jsonl\" \\\n",
    "    --text-column \"text\" \\\n",
    "    --peft \\\n",
    "    --quantization \"int4\"\\\n",
    "    --lora-r 16 \\\n",
    "    --lr 2e-4 \\\n",
    "    --auto-find-batch-size \\\n",
    "    --epochs 1 \\\n",
    "    --trainer sft \\\n",
    "    --block_size 4096 \\\n",
    "    --model_max_length 4096"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "train",
   "language": "python",
   "name": "train"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

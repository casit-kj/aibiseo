{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ae66a6d-b603-457d-8c45-02f29916bee2",
   "metadata": {},
   "source": [
    "#### Llama2 모델을 Quantization(양자화) 모델로 변환\n",
    "   > ##### Llama2-7B 모델은 GPU T4 28G 이상에서 정상 작동을 한다.\n",
    "   \n",
    "1. Llama2 모델을 HuggingFace 모델로 변환한다.\n",
    "2. AutoGPTQ 라이브러리를 사용하여 8bit 양자화 모델을 생성한다.\n",
    "3. https://github.com/jongmin-oh/korean-LLM-quantize/blob/main/quant_with_LLM.py\n",
    "4. 저장파일이름: quant_with_llm.py\n",
    "5. quant_with_llm.py 파일 안에 학습데이터 설정 부분이 존재한다.(en_alpaca_data.json)\n",
    "6. 실행방법\n",
    "   > python quant_with_llm.py \\\n",
    "   > --pretrained_model_dir /data/bwllm/models/casllm-base-7b-hf \\\n",
    "   > --quantized_model_dir /data/bwllm/models/casllm-base-7b-hf-8bit\n",
    "\n",
    "5. python module 설치 (src/requirements.txt)\n",
    "   * %pip install torch fairscale fire sentencepiece transformers protobuf accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407aea90",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r script/requirements.txt -q\n",
    "%pip install auto-gptq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "589c6fe9-c27e-4987-9c54-76d4725eb9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:06<00:00,  2.16s/it]\n",
      "Generating train split: 128 examples [00:00, 5546.70 examples/s]\n",
      "Map: 100%|████████████████████████████| 128/128 [00:00<00:00, 916.60 examples/s]\n",
      "INFO - Start quantizing layer 1/32\n",
      "2024-03-04 11:06:42 INFO [auto_gptq.modeling._base] Start quantizing layer 1/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 1/32...\n",
      "2024-03-04 11:06:44 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 1/32...\n",
      "2024-03-04 11:06:46 INFO [auto_gptq.quantization.gptq] duration: 1.9054052829742432\n",
      "2024-03-04 11:06:46 INFO [auto_gptq.quantization.gptq] avg loss: 0.00021341629326343536\n",
      "INFO - Quantizing self_attn.v_proj in layer 1/32...\n",
      "2024-03-04 11:06:46 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 1/32...\n",
      "2024-03-04 11:06:48 INFO [auto_gptq.quantization.gptq] duration: 1.6820156574249268\n",
      "2024-03-04 11:06:48 INFO [auto_gptq.quantization.gptq] avg loss: 1.0812333130161278e-05\n",
      "INFO - Quantizing self_attn.q_proj in layer 1/32...\n",
      "2024-03-04 11:06:48 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 1/32...\n",
      "2024-03-04 11:06:49 INFO [auto_gptq.quantization.gptq] duration: 1.5356950759887695\n",
      "2024-03-04 11:06:49 INFO [auto_gptq.quantization.gptq] avg loss: 0.00022912585700396448\n",
      "INFO - Quantizing self_attn.o_proj in layer 1/32...\n",
      "2024-03-04 11:06:50 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 1/32...\n",
      "2024-03-04 11:06:51 INFO [auto_gptq.quantization.gptq] duration: 1.6194980144500732\n",
      "2024-03-04 11:06:51 INFO [auto_gptq.quantization.gptq] avg loss: 2.8005644026052323e-07\n",
      "INFO - Quantizing mlp.up_proj in layer 1/32...\n",
      "2024-03-04 11:06:52 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 1/32...\n",
      "2024-03-04 11:06:54 INFO [auto_gptq.quantization.gptq] duration: 1.919499158859253\n",
      "2024-03-04 11:06:54 INFO [auto_gptq.quantization.gptq] avg loss: 0.00012623729708138853\n",
      "INFO - Quantizing mlp.gate_proj in layer 1/32...\n",
      "2024-03-04 11:06:54 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 1/32...\n",
      "2024-03-04 11:06:56 INFO [auto_gptq.quantization.gptq] duration: 1.690493106842041\n",
      "2024-03-04 11:06:56 INFO [auto_gptq.quantization.gptq] avg loss: 0.00012950511882081628\n",
      "INFO - Quantizing mlp.down_proj in layer 1/32...\n",
      "2024-03-04 11:06:58 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 1/32...\n",
      "2024-03-04 11:07:04 INFO [auto_gptq.quantization.gptq] duration: 5.545781373977661\n",
      "2024-03-04 11:07:04 INFO [auto_gptq.quantization.gptq] avg loss: 8.245542630902492e-07\n",
      "INFO - Start quantizing layer 2/32\n",
      "2024-03-04 11:07:05 INFO [auto_gptq.modeling._base] Start quantizing layer 2/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 2/32...\n",
      "2024-03-04 11:07:06 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 2/32...\n",
      "2024-03-04 11:07:08 INFO [auto_gptq.quantization.gptq] duration: 1.7805461883544922\n",
      "2024-03-04 11:07:08 INFO [auto_gptq.quantization.gptq] avg loss: 0.0007458832114934921\n",
      "INFO - Quantizing self_attn.v_proj in layer 2/32...\n",
      "2024-03-04 11:07:08 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 2/32...\n",
      "2024-03-04 11:07:09 INFO [auto_gptq.quantization.gptq] duration: 1.5076110363006592\n",
      "2024-03-04 11:07:09 INFO [auto_gptq.quantization.gptq] avg loss: 4.864469156018458e-05\n",
      "INFO - Quantizing self_attn.q_proj in layer 2/32...\n",
      "2024-03-04 11:07:09 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 2/32...\n",
      "2024-03-04 11:07:11 INFO [auto_gptq.quantization.gptq] duration: 1.5067036151885986\n",
      "2024-03-04 11:07:11 INFO [auto_gptq.quantization.gptq] avg loss: 0.0007188096642494202\n",
      "INFO - Quantizing self_attn.o_proj in layer 2/32...\n",
      "2024-03-04 11:07:11 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 2/32...\n",
      "2024-03-04 11:07:13 INFO [auto_gptq.quantization.gptq] duration: 1.6179256439208984\n",
      "2024-03-04 11:07:13 INFO [auto_gptq.quantization.gptq] avg loss: 4.319334038882516e-06\n",
      "INFO - Quantizing mlp.up_proj in layer 2/32...\n",
      "2024-03-04 11:07:14 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 2/32...\n",
      "2024-03-04 11:07:15 INFO [auto_gptq.quantization.gptq] duration: 1.8545048236846924\n",
      "2024-03-04 11:07:15 INFO [auto_gptq.quantization.gptq] avg loss: 0.0005641603493131697\n",
      "INFO - Quantizing mlp.gate_proj in layer 2/32...\n",
      "2024-03-04 11:07:15 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 2/32...\n",
      "2024-03-04 11:07:17 INFO [auto_gptq.quantization.gptq] duration: 1.6803257465362549\n",
      "2024-03-04 11:07:17 INFO [auto_gptq.quantization.gptq] avg loss: 0.0006307614967226982\n",
      "INFO - Quantizing mlp.down_proj in layer 2/32...\n",
      "2024-03-04 11:07:19 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 2/32...\n",
      "2024-03-04 11:07:25 INFO [auto_gptq.quantization.gptq] duration: 5.5296125411987305\n",
      "2024-03-04 11:07:25 INFO [auto_gptq.quantization.gptq] avg loss: 0.010604240000247955\n",
      "INFO - Start quantizing layer 3/32\n",
      "2024-03-04 11:07:26 INFO [auto_gptq.modeling._base] Start quantizing layer 3/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 3/32...\n",
      "2024-03-04 11:07:27 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 3/32...\n",
      "2024-03-04 11:07:29 INFO [auto_gptq.quantization.gptq] duration: 1.6904325485229492\n",
      "2024-03-04 11:07:29 INFO [auto_gptq.quantization.gptq] avg loss: 0.0023624664172530174\n",
      "INFO - Quantizing self_attn.v_proj in layer 3/32...\n",
      "2024-03-04 11:07:29 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 3/32...\n",
      "2024-03-04 11:07:30 INFO [auto_gptq.quantization.gptq] duration: 1.5306427478790283\n",
      "2024-03-04 11:07:30 INFO [auto_gptq.quantization.gptq] avg loss: 0.000623967731371522\n",
      "INFO - Quantizing self_attn.q_proj in layer 3/32...\n",
      "2024-03-04 11:07:30 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 3/32...\n",
      "2024-03-04 11:07:32 INFO [auto_gptq.quantization.gptq] duration: 1.5543639659881592\n",
      "2024-03-04 11:07:32 INFO [auto_gptq.quantization.gptq] avg loss: 0.00220974488183856\n",
      "INFO - Quantizing self_attn.o_proj in layer 3/32...\n",
      "2024-03-04 11:07:32 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 3/32...\n",
      "2024-03-04 11:07:34 INFO [auto_gptq.quantization.gptq] duration: 1.6510770320892334\n",
      "2024-03-04 11:07:34 INFO [auto_gptq.quantization.gptq] avg loss: 8.910026735975407e-06\n",
      "INFO - Quantizing mlp.up_proj in layer 3/32...\n",
      "2024-03-04 11:07:35 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 3/32...\n",
      "2024-03-04 11:07:37 INFO [auto_gptq.quantization.gptq] duration: 1.9053006172180176\n",
      "2024-03-04 11:07:37 INFO [auto_gptq.quantization.gptq] avg loss: 0.0015359590761363506\n",
      "INFO - Quantizing mlp.gate_proj in layer 3/32...\n",
      "2024-03-04 11:07:37 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 3/32...\n",
      "2024-03-04 11:07:39 INFO [auto_gptq.quantization.gptq] duration: 1.6854355335235596\n",
      "2024-03-04 11:07:39 INFO [auto_gptq.quantization.gptq] avg loss: 0.001759922131896019\n",
      "INFO - Quantizing mlp.down_proj in layer 3/32...\n",
      "2024-03-04 11:07:41 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 3/32...\n",
      "2024-03-04 11:07:46 INFO [auto_gptq.quantization.gptq] duration: 5.519239664077759\n",
      "2024-03-04 11:07:46 INFO [auto_gptq.quantization.gptq] avg loss: 1.4455423297476955e-05\n",
      "INFO - Start quantizing layer 4/32\n",
      "2024-03-04 11:07:47 INFO [auto_gptq.modeling._base] Start quantizing layer 4/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 4/32...\n",
      "2024-03-04 11:07:48 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 4/32...\n",
      "2024-03-04 11:07:50 INFO [auto_gptq.quantization.gptq] duration: 1.7981553077697754\n",
      "2024-03-04 11:07:50 INFO [auto_gptq.quantization.gptq] avg loss: 0.0071930233389139175\n",
      "INFO - Quantizing self_attn.v_proj in layer 4/32...\n",
      "2024-03-04 11:07:50 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 4/32...\n",
      "2024-03-04 11:07:52 INFO [auto_gptq.quantization.gptq] duration: 1.5290424823760986\n",
      "2024-03-04 11:07:52 INFO [auto_gptq.quantization.gptq] avg loss: 0.0019591879099607468\n",
      "INFO - Quantizing self_attn.q_proj in layer 4/32...\n",
      "2024-03-04 11:07:52 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 4/32...\n",
      "2024-03-04 11:07:53 INFO [auto_gptq.quantization.gptq] duration: 1.520026683807373\n",
      "2024-03-04 11:07:53 INFO [auto_gptq.quantization.gptq] avg loss: 0.007019154727458954\n",
      "INFO - Quantizing self_attn.o_proj in layer 4/32...\n",
      "2024-03-04 11:07:54 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 4/32...\n",
      "2024-03-04 11:07:56 INFO [auto_gptq.quantization.gptq] duration: 1.6271131038665771\n",
      "2024-03-04 11:07:56 INFO [auto_gptq.quantization.gptq] avg loss: 1.3084450984024443e-05\n",
      "INFO - Quantizing mlp.up_proj in layer 4/32...\n",
      "2024-03-04 11:07:57 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 4/32...\n",
      "2024-03-04 11:07:58 INFO [auto_gptq.quantization.gptq] duration: 1.824376106262207\n",
      "2024-03-04 11:07:58 INFO [auto_gptq.quantization.gptq] avg loss: 0.00278687896206975\n",
      "INFO - Quantizing mlp.gate_proj in layer 4/32...\n",
      "2024-03-04 11:07:58 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 4/32...\n",
      "2024-03-04 11:08:00 INFO [auto_gptq.quantization.gptq] duration: 1.6715846061706543\n",
      "2024-03-04 11:08:00 INFO [auto_gptq.quantization.gptq] avg loss: 0.00323410052806139\n",
      "INFO - Quantizing mlp.down_proj in layer 4/32...\n",
      "2024-03-04 11:08:02 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 4/32...\n",
      "2024-03-04 11:08:08 INFO [auto_gptq.quantization.gptq] duration: 5.5283777713775635\n",
      "2024-03-04 11:08:08 INFO [auto_gptq.quantization.gptq] avg loss: 3.2069703593151644e-05\n",
      "INFO - Start quantizing layer 5/32\n",
      "2024-03-04 11:08:09 INFO [auto_gptq.modeling._base] Start quantizing layer 5/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 5/32...\n",
      "2024-03-04 11:08:10 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 5/32...\n",
      "2024-03-04 11:08:12 INFO [auto_gptq.quantization.gptq] duration: 1.7680480480194092\n",
      "2024-03-04 11:08:12 INFO [auto_gptq.quantization.gptq] avg loss: 0.007039614021778107\n",
      "INFO - Quantizing self_attn.v_proj in layer 5/32...\n",
      "2024-03-04 11:08:12 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 5/32...\n",
      "2024-03-04 11:08:13 INFO [auto_gptq.quantization.gptq] duration: 1.5567104816436768\n",
      "2024-03-04 11:08:13 INFO [auto_gptq.quantization.gptq] avg loss: 0.0020089484751224518\n",
      "INFO - Quantizing self_attn.q_proj in layer 5/32...\n",
      "2024-03-04 11:08:13 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 5/32...\n",
      "2024-03-04 11:08:15 INFO [auto_gptq.quantization.gptq] duration: 1.5311212539672852\n",
      "2024-03-04 11:08:15 INFO [auto_gptq.quantization.gptq] avg loss: 0.007323344238102436\n",
      "INFO - Quantizing self_attn.o_proj in layer 5/32...\n",
      "2024-03-04 11:08:15 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 5/32...\n",
      "2024-03-04 11:08:17 INFO [auto_gptq.quantization.gptq] duration: 1.661409616470337\n",
      "2024-03-04 11:08:17 INFO [auto_gptq.quantization.gptq] avg loss: 2.6663146854843944e-05\n",
      "INFO - Quantizing mlp.up_proj in layer 5/32...\n",
      "2024-03-04 11:08:18 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 5/32...\n",
      "2024-03-04 11:08:20 INFO [auto_gptq.quantization.gptq] duration: 1.8389461040496826\n",
      "2024-03-04 11:08:20 INFO [auto_gptq.quantization.gptq] avg loss: 0.003659597598016262\n",
      "INFO - Quantizing mlp.gate_proj in layer 5/32...\n",
      "2024-03-04 11:08:20 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 5/32...\n",
      "2024-03-04 11:08:21 INFO [auto_gptq.quantization.gptq] duration: 1.6795368194580078\n",
      "2024-03-04 11:08:21 INFO [auto_gptq.quantization.gptq] avg loss: 0.004490627441555262\n",
      "INFO - Quantizing mlp.down_proj in layer 5/32...\n",
      "2024-03-04 11:08:23 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 5/32...\n",
      "2024-03-04 11:08:29 INFO [auto_gptq.quantization.gptq] duration: 5.579030752182007\n",
      "2024-03-04 11:08:29 INFO [auto_gptq.quantization.gptq] avg loss: 6.807809404563159e-05\n",
      "INFO - Start quantizing layer 6/32\n",
      "2024-03-04 11:08:30 INFO [auto_gptq.modeling._base] Start quantizing layer 6/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 6/32...\n",
      "2024-03-04 11:08:31 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 6/32...\n",
      "2024-03-04 11:08:33 INFO [auto_gptq.quantization.gptq] duration: 1.7991302013397217\n",
      "2024-03-04 11:08:33 INFO [auto_gptq.quantization.gptq] avg loss: 0.008392740041017532\n",
      "INFO - Quantizing self_attn.v_proj in layer 6/32...\n",
      "2024-03-04 11:08:33 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 6/32...\n",
      "2024-03-04 11:08:34 INFO [auto_gptq.quantization.gptq] duration: 1.534609079360962\n",
      "2024-03-04 11:08:34 INFO [auto_gptq.quantization.gptq] avg loss: 0.002422940917313099\n",
      "INFO - Quantizing self_attn.q_proj in layer 6/32...\n",
      "2024-03-04 11:08:34 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 6/32...\n",
      "2024-03-04 11:08:36 INFO [auto_gptq.quantization.gptq] duration: 1.5256078243255615\n",
      "2024-03-04 11:08:36 INFO [auto_gptq.quantization.gptq] avg loss: 0.008016270585358143\n",
      "INFO - Quantizing self_attn.o_proj in layer 6/32...\n",
      "2024-03-04 11:08:37 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 6/32...\n",
      "2024-03-04 11:08:38 INFO [auto_gptq.quantization.gptq] duration: 1.6394133567810059\n",
      "2024-03-04 11:08:38 INFO [auto_gptq.quantization.gptq] avg loss: 5.537778270081617e-05\n",
      "INFO - Quantizing mlp.up_proj in layer 6/32...\n",
      "2024-03-04 11:08:39 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 6/32...\n",
      "2024-03-04 11:08:41 INFO [auto_gptq.quantization.gptq] duration: 1.8282697200775146\n",
      "2024-03-04 11:08:41 INFO [auto_gptq.quantization.gptq] avg loss: 0.0045028324238955975\n",
      "INFO - Quantizing mlp.gate_proj in layer 6/32...\n",
      "2024-03-04 11:08:41 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 6/32...\n",
      "2024-03-04 11:08:43 INFO [auto_gptq.quantization.gptq] duration: 1.6872148513793945\n",
      "2024-03-04 11:08:43 INFO [auto_gptq.quantization.gptq] avg loss: 0.005572792142629623\n",
      "INFO - Quantizing mlp.down_proj in layer 6/32...\n",
      "2024-03-04 11:08:45 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 6/32...\n",
      "2024-03-04 11:08:50 INFO [auto_gptq.quantization.gptq] duration: 5.5966761112213135\n",
      "2024-03-04 11:08:50 INFO [auto_gptq.quantization.gptq] avg loss: 9.790858894120902e-05\n",
      "INFO - Start quantizing layer 7/32\n",
      "2024-03-04 11:08:51 INFO [auto_gptq.modeling._base] Start quantizing layer 7/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 7/32...\n",
      "2024-03-04 11:08:53 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 7/32...\n",
      "2024-03-04 11:08:54 INFO [auto_gptq.quantization.gptq] duration: 1.6896100044250488\n",
      "2024-03-04 11:08:54 INFO [auto_gptq.quantization.gptq] avg loss: 0.011324360966682434\n",
      "INFO - Quantizing self_attn.v_proj in layer 7/32...\n",
      "2024-03-04 11:08:54 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 7/32...\n",
      "2024-03-04 11:08:56 INFO [auto_gptq.quantization.gptq] duration: 1.529625415802002\n",
      "2024-03-04 11:08:56 INFO [auto_gptq.quantization.gptq] avg loss: 0.003323556389659643\n",
      "INFO - Quantizing self_attn.q_proj in layer 7/32...\n",
      "2024-03-04 11:08:56 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 7/32...\n",
      "2024-03-04 11:08:57 INFO [auto_gptq.quantization.gptq] duration: 1.541715145111084\n",
      "2024-03-04 11:08:57 INFO [auto_gptq.quantization.gptq] avg loss: 0.01142428070306778\n",
      "INFO - Quantizing self_attn.o_proj in layer 7/32...\n",
      "2024-03-04 11:08:58 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 7/32...\n",
      "2024-03-04 11:09:00 INFO [auto_gptq.quantization.gptq] duration: 1.6325743198394775\n",
      "2024-03-04 11:09:00 INFO [auto_gptq.quantization.gptq] avg loss: 5.474656063597649e-05\n",
      "INFO - Quantizing mlp.up_proj in layer 7/32...\n",
      "2024-03-04 11:09:01 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 7/32...\n",
      "2024-03-04 11:09:02 INFO [auto_gptq.quantization.gptq] duration: 1.8262262344360352\n",
      "2024-03-04 11:09:02 INFO [auto_gptq.quantization.gptq] avg loss: 0.005444644950330257\n",
      "INFO - Quantizing mlp.gate_proj in layer 7/32...\n",
      "2024-03-04 11:09:02 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 7/32...\n",
      "2024-03-04 11:09:04 INFO [auto_gptq.quantization.gptq] duration: 1.6713929176330566\n",
      "2024-03-04 11:09:04 INFO [auto_gptq.quantization.gptq] avg loss: 0.0070176078006625175\n",
      "INFO - Quantizing mlp.down_proj in layer 7/32...\n",
      "2024-03-04 11:09:06 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 7/32...\n",
      "2024-03-04 11:09:12 INFO [auto_gptq.quantization.gptq] duration: 5.555125713348389\n",
      "2024-03-04 11:09:12 INFO [auto_gptq.quantization.gptq] avg loss: 0.0001421864581061527\n",
      "INFO - Start quantizing layer 8/32\n",
      "2024-03-04 11:09:13 INFO [auto_gptq.modeling._base] Start quantizing layer 8/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 8/32...\n",
      "2024-03-04 11:09:14 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 8/32...\n",
      "2024-03-04 11:09:16 INFO [auto_gptq.quantization.gptq] duration: 1.7759082317352295\n",
      "2024-03-04 11:09:16 INFO [auto_gptq.quantization.gptq] avg loss: 0.011795494705438614\n",
      "INFO - Quantizing self_attn.v_proj in layer 8/32...\n",
      "2024-03-04 11:09:16 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 8/32...\n",
      "2024-03-04 11:09:17 INFO [auto_gptq.quantization.gptq] duration: 1.530808448791504\n",
      "2024-03-04 11:09:17 INFO [auto_gptq.quantization.gptq] avg loss: 0.003682160982862115\n",
      "INFO - Quantizing self_attn.q_proj in layer 8/32...\n",
      "2024-03-04 11:09:17 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 8/32...\n",
      "2024-03-04 11:09:19 INFO [auto_gptq.quantization.gptq] duration: 1.522444486618042\n",
      "2024-03-04 11:09:19 INFO [auto_gptq.quantization.gptq] avg loss: 0.012058313935995102\n",
      "INFO - Quantizing self_attn.o_proj in layer 8/32...\n",
      "2024-03-04 11:09:19 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 8/32...\n",
      "2024-03-04 11:09:21 INFO [auto_gptq.quantization.gptq] duration: 1.6229090690612793\n",
      "2024-03-04 11:09:21 INFO [auto_gptq.quantization.gptq] avg loss: 7.619727693963796e-05\n",
      "INFO - Quantizing mlp.up_proj in layer 8/32...\n",
      "2024-03-04 11:09:22 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 8/32...\n",
      "2024-03-04 11:09:24 INFO [auto_gptq.quantization.gptq] duration: 1.671893835067749\n",
      "2024-03-04 11:09:24 INFO [auto_gptq.quantization.gptq] avg loss: 0.006215323694050312\n",
      "INFO - Quantizing mlp.gate_proj in layer 8/32...\n",
      "2024-03-04 11:09:24 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 8/32...\n",
      "2024-03-04 11:09:25 INFO [auto_gptq.quantization.gptq] duration: 1.6649749279022217\n",
      "2024-03-04 11:09:25 INFO [auto_gptq.quantization.gptq] avg loss: 0.007954399101436138\n",
      "INFO - Quantizing mlp.down_proj in layer 8/32...\n",
      "2024-03-04 11:09:27 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 8/32...\n",
      "2024-03-04 11:09:33 INFO [auto_gptq.quantization.gptq] duration: 5.5292274951934814\n",
      "2024-03-04 11:09:33 INFO [auto_gptq.quantization.gptq] avg loss: 0.00019191978208255023\n",
      "INFO - Start quantizing layer 9/32\n",
      "2024-03-04 11:09:34 INFO [auto_gptq.modeling._base] Start quantizing layer 9/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 9/32...\n",
      "2024-03-04 11:09:35 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 9/32...\n",
      "2024-03-04 11:09:37 INFO [auto_gptq.quantization.gptq] duration: 1.797900676727295\n",
      "2024-03-04 11:09:37 INFO [auto_gptq.quantization.gptq] avg loss: 0.011754182167351246\n",
      "INFO - Quantizing self_attn.v_proj in layer 9/32...\n",
      "2024-03-04 11:09:37 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 9/32...\n",
      "2024-03-04 11:09:38 INFO [auto_gptq.quantization.gptq] duration: 1.5247132778167725\n",
      "2024-03-04 11:09:38 INFO [auto_gptq.quantization.gptq] avg loss: 0.003735547186806798\n",
      "INFO - Quantizing self_attn.q_proj in layer 9/32...\n",
      "2024-03-04 11:09:38 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 9/32...\n",
      "2024-03-04 11:09:40 INFO [auto_gptq.quantization.gptq] duration: 1.5291597843170166\n",
      "2024-03-04 11:09:40 INFO [auto_gptq.quantization.gptq] avg loss: 0.01181928813457489\n",
      "INFO - Quantizing self_attn.o_proj in layer 9/32...\n",
      "2024-03-04 11:09:41 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 9/32...\n",
      "2024-03-04 11:09:42 INFO [auto_gptq.quantization.gptq] duration: 1.6282095909118652\n",
      "2024-03-04 11:09:42 INFO [auto_gptq.quantization.gptq] avg loss: 0.00011006242129951715\n",
      "INFO - Quantizing mlp.up_proj in layer 9/32...\n",
      "2024-03-04 11:09:43 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 9/32...\n",
      "2024-03-04 11:09:45 INFO [auto_gptq.quantization.gptq] duration: 1.7034974098205566\n",
      "2024-03-04 11:09:45 INFO [auto_gptq.quantization.gptq] avg loss: 0.006753625348210335\n",
      "INFO - Quantizing mlp.gate_proj in layer 9/32...\n",
      "2024-03-04 11:09:45 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 9/32...\n",
      "2024-03-04 11:09:47 INFO [auto_gptq.quantization.gptq] duration: 1.6805238723754883\n",
      "2024-03-04 11:09:47 INFO [auto_gptq.quantization.gptq] avg loss: 0.008175462484359741\n",
      "INFO - Quantizing mlp.down_proj in layer 9/32...\n",
      "2024-03-04 11:09:49 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 9/32...\n",
      "2024-03-04 11:09:54 INFO [auto_gptq.quantization.gptq] duration: 5.545297384262085\n",
      "2024-03-04 11:09:54 INFO [auto_gptq.quantization.gptq] avg loss: 0.0002403847174718976\n",
      "INFO - Start quantizing layer 10/32\n",
      "2024-03-04 11:09:55 INFO [auto_gptq.modeling._base] Start quantizing layer 10/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 10/32...\n",
      "2024-03-04 11:09:57 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 10/32...\n",
      "2024-03-04 11:09:58 INFO [auto_gptq.quantization.gptq] duration: 1.7803237438201904\n",
      "2024-03-04 11:09:58 INFO [auto_gptq.quantization.gptq] avg loss: 0.013065378181636333\n",
      "INFO - Quantizing self_attn.v_proj in layer 10/32...\n",
      "2024-03-04 11:09:58 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 10/32...\n",
      "2024-03-04 11:10:00 INFO [auto_gptq.quantization.gptq] duration: 1.516289472579956\n",
      "2024-03-04 11:10:00 INFO [auto_gptq.quantization.gptq] avg loss: 0.004166252911090851\n",
      "INFO - Quantizing self_attn.q_proj in layer 10/32...\n",
      "2024-03-04 11:10:00 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 10/32...\n",
      "2024-03-04 11:10:01 INFO [auto_gptq.quantization.gptq] duration: 1.511385440826416\n",
      "2024-03-04 11:10:01 INFO [auto_gptq.quantization.gptq] avg loss: 0.012604224495589733\n",
      "INFO - Quantizing self_attn.o_proj in layer 10/32...\n",
      "2024-03-04 11:10:02 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 10/32...\n",
      "2024-03-04 11:10:04 INFO [auto_gptq.quantization.gptq] duration: 1.6279783248901367\n",
      "2024-03-04 11:10:04 INFO [auto_gptq.quantization.gptq] avg loss: 0.000166576006449759\n",
      "INFO - Quantizing mlp.up_proj in layer 10/32...\n",
      "2024-03-04 11:10:05 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 10/32...\n",
      "2024-03-04 11:10:06 INFO [auto_gptq.quantization.gptq] duration: 1.821624517440796\n",
      "2024-03-04 11:10:06 INFO [auto_gptq.quantization.gptq] avg loss: 0.007399027235805988\n",
      "INFO - Quantizing mlp.gate_proj in layer 10/32...\n",
      "2024-03-04 11:10:06 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 10/32...\n",
      "2024-03-04 11:10:08 INFO [auto_gptq.quantization.gptq] duration: 1.6621747016906738\n",
      "2024-03-04 11:10:08 INFO [auto_gptq.quantization.gptq] avg loss: 0.008654998615384102\n",
      "INFO - Quantizing mlp.down_proj in layer 10/32...\n",
      "2024-03-04 11:10:10 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 10/32...\n",
      "2024-03-04 11:10:16 INFO [auto_gptq.quantization.gptq] duration: 5.5199360847473145\n",
      "2024-03-04 11:10:16 INFO [auto_gptq.quantization.gptq] avg loss: 0.0002833520993590355\n",
      "INFO - Start quantizing layer 11/32\n",
      "2024-03-04 11:10:16 INFO [auto_gptq.modeling._base] Start quantizing layer 11/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 11/32...\n",
      "2024-03-04 11:10:18 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 11/32...\n",
      "2024-03-04 11:10:20 INFO [auto_gptq.quantization.gptq] duration: 1.7973120212554932\n",
      "2024-03-04 11:10:20 INFO [auto_gptq.quantization.gptq] avg loss: 0.013711529783904552\n",
      "INFO - Quantizing self_attn.v_proj in layer 11/32...\n",
      "2024-03-04 11:10:20 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 11/32...\n",
      "2024-03-04 11:10:21 INFO [auto_gptq.quantization.gptq] duration: 1.5297703742980957\n",
      "2024-03-04 11:10:21 INFO [auto_gptq.quantization.gptq] avg loss: 0.0042834714986383915\n",
      "INFO - Quantizing self_attn.q_proj in layer 11/32...\n",
      "2024-03-04 11:10:21 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 11/32...\n",
      "2024-03-04 11:10:23 INFO [auto_gptq.quantization.gptq] duration: 1.5319263935089111\n",
      "2024-03-04 11:10:23 INFO [auto_gptq.quantization.gptq] avg loss: 0.012972638010978699\n",
      "INFO - Quantizing self_attn.o_proj in layer 11/32...\n",
      "2024-03-04 11:10:23 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 11/32...\n",
      "2024-03-04 11:10:25 INFO [auto_gptq.quantization.gptq] duration: 1.6302201747894287\n",
      "2024-03-04 11:10:25 INFO [auto_gptq.quantization.gptq] avg loss: 0.0001974260958377272\n",
      "INFO - Quantizing mlp.up_proj in layer 11/32...\n",
      "2024-03-04 11:10:26 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 11/32...\n",
      "2024-03-04 11:10:28 INFO [auto_gptq.quantization.gptq] duration: 1.714080810546875\n",
      "2024-03-04 11:10:28 INFO [auto_gptq.quantization.gptq] avg loss: 0.007820802740752697\n",
      "INFO - Quantizing mlp.gate_proj in layer 11/32...\n",
      "2024-03-04 11:10:28 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 11/32...\n",
      "2024-03-04 11:10:29 INFO [auto_gptq.quantization.gptq] duration: 1.6621880531311035\n",
      "2024-03-04 11:10:29 INFO [auto_gptq.quantization.gptq] avg loss: 0.008927788585424423\n",
      "INFO - Quantizing mlp.down_proj in layer 11/32...\n",
      "2024-03-04 11:10:31 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 11/32...\n",
      "2024-03-04 11:10:37 INFO [auto_gptq.quantization.gptq] duration: 5.554006338119507\n",
      "2024-03-04 11:10:37 INFO [auto_gptq.quantization.gptq] avg loss: 0.0003166046808473766\n",
      "INFO - Start quantizing layer 12/32\n",
      "2024-03-04 11:10:38 INFO [auto_gptq.modeling._base] Start quantizing layer 12/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 12/32...\n",
      "2024-03-04 11:10:39 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 12/32...\n",
      "2024-03-04 11:10:41 INFO [auto_gptq.quantization.gptq] duration: 1.8060634136199951\n",
      "2024-03-04 11:10:41 INFO [auto_gptq.quantization.gptq] avg loss: 0.013913748785853386\n",
      "INFO - Quantizing self_attn.v_proj in layer 12/32...\n",
      "2024-03-04 11:10:41 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 12/32...\n",
      "2024-03-04 11:10:43 INFO [auto_gptq.quantization.gptq] duration: 1.5341401100158691\n",
      "2024-03-04 11:10:43 INFO [auto_gptq.quantization.gptq] avg loss: 0.0056235468946397305\n",
      "INFO - Quantizing self_attn.q_proj in layer 12/32...\n",
      "2024-03-04 11:10:43 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 12/32...\n",
      "2024-03-04 11:10:44 INFO [auto_gptq.quantization.gptq] duration: 1.5216634273529053\n",
      "2024-03-04 11:10:44 INFO [auto_gptq.quantization.gptq] avg loss: 0.01414349302649498\n",
      "INFO - Quantizing self_attn.o_proj in layer 12/32...\n",
      "2024-03-04 11:10:45 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 12/32...\n",
      "2024-03-04 11:10:46 INFO [auto_gptq.quantization.gptq] duration: 1.6418912410736084\n",
      "2024-03-04 11:10:46 INFO [auto_gptq.quantization.gptq] avg loss: 0.00018898501002695411\n",
      "INFO - Quantizing mlp.up_proj in layer 12/32...\n",
      "2024-03-04 11:10:47 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 12/32...\n",
      "2024-03-04 11:10:49 INFO [auto_gptq.quantization.gptq] duration: 1.8564000129699707\n",
      "2024-03-04 11:10:49 INFO [auto_gptq.quantization.gptq] avg loss: 0.008560743182897568\n",
      "INFO - Quantizing mlp.gate_proj in layer 12/32...\n",
      "2024-03-04 11:10:49 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 12/32...\n",
      "2024-03-04 11:10:51 INFO [auto_gptq.quantization.gptq] duration: 1.7179756164550781\n",
      "2024-03-04 11:10:51 INFO [auto_gptq.quantization.gptq] avg loss: 0.009527555666863918\n",
      "INFO - Quantizing mlp.down_proj in layer 12/32...\n",
      "2024-03-04 11:10:53 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 12/32...\n",
      "2024-03-04 11:10:59 INFO [auto_gptq.quantization.gptq] duration: 5.6694395542144775\n",
      "2024-03-04 11:10:59 INFO [auto_gptq.quantization.gptq] avg loss: 0.0003576402086764574\n",
      "INFO - Start quantizing layer 13/32\n",
      "2024-03-04 11:11:00 INFO [auto_gptq.modeling._base] Start quantizing layer 13/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 13/32...\n",
      "2024-03-04 11:11:01 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 13/32...\n",
      "2024-03-04 11:11:02 INFO [auto_gptq.quantization.gptq] duration: 1.6880497932434082\n",
      "2024-03-04 11:11:02 INFO [auto_gptq.quantization.gptq] avg loss: 0.01557263731956482\n",
      "INFO - Quantizing self_attn.v_proj in layer 13/32...\n",
      "2024-03-04 11:11:02 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 13/32...\n",
      "2024-03-04 11:11:04 INFO [auto_gptq.quantization.gptq] duration: 1.5122253894805908\n",
      "2024-03-04 11:11:04 INFO [auto_gptq.quantization.gptq] avg loss: 0.005511781200766563\n",
      "INFO - Quantizing self_attn.q_proj in layer 13/32...\n",
      "2024-03-04 11:11:04 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 13/32...\n",
      "2024-03-04 11:11:06 INFO [auto_gptq.quantization.gptq] duration: 1.5194165706634521\n",
      "2024-03-04 11:11:06 INFO [auto_gptq.quantization.gptq] avg loss: 0.014801807701587677\n",
      "INFO - Quantizing self_attn.o_proj in layer 13/32...\n",
      "2024-03-04 11:11:06 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 13/32...\n",
      "2024-03-04 11:11:08 INFO [auto_gptq.quantization.gptq] duration: 1.6719262599945068\n",
      "2024-03-04 11:11:08 INFO [auto_gptq.quantization.gptq] avg loss: 0.00023698675795458257\n",
      "INFO - Quantizing mlp.up_proj in layer 13/32...\n",
      "2024-03-04 11:11:09 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 13/32...\n",
      "2024-03-04 11:11:11 INFO [auto_gptq.quantization.gptq] duration: 1.8294477462768555\n",
      "2024-03-04 11:11:11 INFO [auto_gptq.quantization.gptq] avg loss: 0.009192240424454212\n",
      "INFO - Quantizing mlp.gate_proj in layer 13/32...\n",
      "2024-03-04 11:11:11 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 13/32...\n",
      "2024-03-04 11:11:12 INFO [auto_gptq.quantization.gptq] duration: 1.6673481464385986\n",
      "2024-03-04 11:11:12 INFO [auto_gptq.quantization.gptq] avg loss: 0.009916212409734726\n",
      "INFO - Quantizing mlp.down_proj in layer 13/32...\n",
      "2024-03-04 11:11:14 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 13/32...\n",
      "2024-03-04 11:11:20 INFO [auto_gptq.quantization.gptq] duration: 5.689699172973633\n",
      "2024-03-04 11:11:20 INFO [auto_gptq.quantization.gptq] avg loss: 0.0004083608218934387\n",
      "INFO - Start quantizing layer 14/32\n",
      "2024-03-04 11:11:21 INFO [auto_gptq.modeling._base] Start quantizing layer 14/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 14/32...\n",
      "2024-03-04 11:11:22 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 14/32...\n",
      "2024-03-04 11:11:24 INFO [auto_gptq.quantization.gptq] duration: 1.6396303176879883\n",
      "2024-03-04 11:11:24 INFO [auto_gptq.quantization.gptq] avg loss: 0.01561812125146389\n",
      "INFO - Quantizing self_attn.v_proj in layer 14/32...\n",
      "2024-03-04 11:11:24 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 14/32...\n",
      "2024-03-04 11:11:25 INFO [auto_gptq.quantization.gptq] duration: 1.5190186500549316\n",
      "2024-03-04 11:11:25 INFO [auto_gptq.quantization.gptq] avg loss: 0.006164822727441788\n",
      "INFO - Quantizing self_attn.q_proj in layer 14/32...\n",
      "2024-03-04 11:11:25 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 14/32...\n",
      "2024-03-04 11:11:27 INFO [auto_gptq.quantization.gptq] duration: 1.5184409618377686\n",
      "2024-03-04 11:11:27 INFO [auto_gptq.quantization.gptq] avg loss: 0.015378579497337341\n",
      "INFO - Quantizing self_attn.o_proj in layer 14/32...\n",
      "2024-03-04 11:11:28 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 14/32...\n",
      "2024-03-04 11:11:29 INFO [auto_gptq.quantization.gptq] duration: 1.688767910003662\n",
      "2024-03-04 11:11:29 INFO [auto_gptq.quantization.gptq] avg loss: 0.0002799778594635427\n",
      "INFO - Quantizing mlp.up_proj in layer 14/32...\n",
      "2024-03-04 11:11:30 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 14/32...\n",
      "2024-03-04 11:11:32 INFO [auto_gptq.quantization.gptq] duration: 1.818288803100586\n",
      "2024-03-04 11:11:32 INFO [auto_gptq.quantization.gptq] avg loss: 0.009807995520532131\n",
      "INFO - Quantizing mlp.gate_proj in layer 14/32...\n",
      "2024-03-04 11:11:32 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 14/32...\n",
      "2024-03-04 11:11:34 INFO [auto_gptq.quantization.gptq] duration: 1.6737730503082275\n",
      "2024-03-04 11:11:34 INFO [auto_gptq.quantization.gptq] avg loss: 0.01031078677624464\n",
      "INFO - Quantizing mlp.down_proj in layer 14/32...\n",
      "2024-03-04 11:11:36 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 14/32...\n",
      "2024-03-04 11:11:41 INFO [auto_gptq.quantization.gptq] duration: 5.665248155593872\n",
      "2024-03-04 11:11:41 INFO [auto_gptq.quantization.gptq] avg loss: 0.0005006187129765749\n",
      "INFO - Start quantizing layer 15/32\n",
      "2024-03-04 11:11:42 INFO [auto_gptq.modeling._base] Start quantizing layer 15/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 15/32...\n",
      "2024-03-04 11:11:43 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 15/32...\n",
      "2024-03-04 11:11:45 INFO [auto_gptq.quantization.gptq] duration: 1.6825783252716064\n",
      "2024-03-04 11:11:45 INFO [auto_gptq.quantization.gptq] avg loss: 0.01575777679681778\n",
      "INFO - Quantizing self_attn.v_proj in layer 15/32...\n",
      "2024-03-04 11:11:45 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 15/32...\n",
      "2024-03-04 11:11:47 INFO [auto_gptq.quantization.gptq] duration: 1.525076150894165\n",
      "2024-03-04 11:11:47 INFO [auto_gptq.quantization.gptq] avg loss: 0.006018222309648991\n",
      "INFO - Quantizing self_attn.q_proj in layer 15/32...\n",
      "2024-03-04 11:11:47 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 15/32...\n",
      "2024-03-04 11:11:48 INFO [auto_gptq.quantization.gptq] duration: 1.5191340446472168\n",
      "2024-03-04 11:11:48 INFO [auto_gptq.quantization.gptq] avg loss: 0.015315745025873184\n",
      "INFO - Quantizing self_attn.o_proj in layer 15/32...\n",
      "2024-03-04 11:11:49 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 15/32...\n",
      "2024-03-04 11:11:51 INFO [auto_gptq.quantization.gptq] duration: 1.6763830184936523\n",
      "2024-03-04 11:11:51 INFO [auto_gptq.quantization.gptq] avg loss: 0.00032005104003474116\n",
      "INFO - Quantizing mlp.up_proj in layer 15/32...\n",
      "2024-03-04 11:11:51 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 15/32...\n",
      "2024-03-04 11:11:53 INFO [auto_gptq.quantization.gptq] duration: 1.8401165008544922\n",
      "2024-03-04 11:11:53 INFO [auto_gptq.quantization.gptq] avg loss: 0.0108043123036623\n",
      "INFO - Quantizing mlp.gate_proj in layer 15/32...\n",
      "2024-03-04 11:11:53 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 15/32...\n",
      "2024-03-04 11:11:55 INFO [auto_gptq.quantization.gptq] duration: 1.685431957244873\n",
      "2024-03-04 11:11:55 INFO [auto_gptq.quantization.gptq] avg loss: 0.011296408250927925\n",
      "INFO - Quantizing mlp.down_proj in layer 15/32...\n",
      "2024-03-04 11:11:57 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 15/32...\n",
      "2024-03-04 11:12:03 INFO [auto_gptq.quantization.gptq] duration: 5.65738582611084\n",
      "2024-03-04 11:12:03 INFO [auto_gptq.quantization.gptq] avg loss: 0.0006015933468006551\n",
      "INFO - Start quantizing layer 16/32\n",
      "2024-03-04 11:12:03 INFO [auto_gptq.modeling._base] Start quantizing layer 16/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 16/32...\n",
      "2024-03-04 11:12:05 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 16/32...\n",
      "2024-03-04 11:12:06 INFO [auto_gptq.quantization.gptq] duration: 1.609025478363037\n",
      "2024-03-04 11:12:06 INFO [auto_gptq.quantization.gptq] avg loss: 0.0156838521361351\n",
      "INFO - Quantizing self_attn.v_proj in layer 16/32...\n",
      "2024-03-04 11:12:06 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 16/32...\n",
      "2024-03-04 11:12:08 INFO [auto_gptq.quantization.gptq] duration: 1.5173184871673584\n",
      "2024-03-04 11:12:08 INFO [auto_gptq.quantization.gptq] avg loss: 0.006416394375264645\n",
      "INFO - Quantizing self_attn.q_proj in layer 16/32...\n",
      "2024-03-04 11:12:08 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 16/32...\n",
      "2024-03-04 11:12:09 INFO [auto_gptq.quantization.gptq] duration: 1.520155906677246\n",
      "2024-03-04 11:12:09 INFO [auto_gptq.quantization.gptq] avg loss: 0.015307840891182423\n",
      "INFO - Quantizing self_attn.o_proj in layer 16/32...\n",
      "2024-03-04 11:12:10 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 16/32...\n",
      "2024-03-04 11:12:12 INFO [auto_gptq.quantization.gptq] duration: 1.6323111057281494\n",
      "2024-03-04 11:12:12 INFO [auto_gptq.quantization.gptq] avg loss: 0.00041173247154802084\n",
      "INFO - Quantizing mlp.up_proj in layer 16/32...\n",
      "2024-03-04 11:12:13 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 16/32...\n",
      "2024-03-04 11:12:14 INFO [auto_gptq.quantization.gptq] duration: 1.831270694732666\n",
      "2024-03-04 11:12:14 INFO [auto_gptq.quantization.gptq] avg loss: 0.011942557990550995\n",
      "INFO - Quantizing mlp.gate_proj in layer 16/32...\n",
      "2024-03-04 11:12:14 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 16/32...\n",
      "2024-03-04 11:12:16 INFO [auto_gptq.quantization.gptq] duration: 1.7178928852081299\n",
      "2024-03-04 11:12:16 INFO [auto_gptq.quantization.gptq] avg loss: 0.012461958453059196\n",
      "INFO - Quantizing mlp.down_proj in layer 16/32...\n",
      "2024-03-04 11:12:18 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 16/32...\n",
      "2024-03-04 11:12:24 INFO [auto_gptq.quantization.gptq] duration: 5.568919897079468\n",
      "2024-03-04 11:12:24 INFO [auto_gptq.quantization.gptq] avg loss: 0.0007721919100731611\n",
      "INFO - Start quantizing layer 17/32\n",
      "2024-03-04 11:12:25 INFO [auto_gptq.modeling._base] Start quantizing layer 17/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 17/32...\n",
      "2024-03-04 11:12:26 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 17/32...\n",
      "2024-03-04 11:12:28 INFO [auto_gptq.quantization.gptq] duration: 1.6949684619903564\n",
      "2024-03-04 11:12:28 INFO [auto_gptq.quantization.gptq] avg loss: 0.016233697533607483\n",
      "INFO - Quantizing self_attn.v_proj in layer 17/32...\n",
      "2024-03-04 11:12:28 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 17/32...\n",
      "2024-03-04 11:12:29 INFO [auto_gptq.quantization.gptq] duration: 1.5277037620544434\n",
      "2024-03-04 11:12:29 INFO [auto_gptq.quantization.gptq] avg loss: 0.007414164952933788\n",
      "INFO - Quantizing self_attn.q_proj in layer 17/32...\n",
      "2024-03-04 11:12:29 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 17/32...\n",
      "2024-03-04 11:12:31 INFO [auto_gptq.quantization.gptq] duration: 1.5207078456878662\n",
      "2024-03-04 11:12:31 INFO [auto_gptq.quantization.gptq] avg loss: 0.016307083889842033\n",
      "INFO - Quantizing self_attn.o_proj in layer 17/32...\n",
      "2024-03-04 11:12:31 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 17/32...\n",
      "2024-03-04 11:12:33 INFO [auto_gptq.quantization.gptq] duration: 1.639066219329834\n",
      "2024-03-04 11:12:33 INFO [auto_gptq.quantization.gptq] avg loss: 0.0005892781773582101\n",
      "INFO - Quantizing mlp.up_proj in layer 17/32...\n",
      "2024-03-04 11:12:34 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 17/32...\n",
      "2024-03-04 11:12:36 INFO [auto_gptq.quantization.gptq] duration: 1.821362018585205\n",
      "2024-03-04 11:12:36 INFO [auto_gptq.quantization.gptq] avg loss: 0.013382147997617722\n",
      "INFO - Quantizing mlp.gate_proj in layer 17/32...\n",
      "2024-03-04 11:12:36 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 17/32...\n",
      "2024-03-04 11:12:37 INFO [auto_gptq.quantization.gptq] duration: 1.6710753440856934\n",
      "2024-03-04 11:12:37 INFO [auto_gptq.quantization.gptq] avg loss: 0.01416019443422556\n",
      "INFO - Quantizing mlp.down_proj in layer 17/32...\n",
      "2024-03-04 11:12:39 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 17/32...\n",
      "2024-03-04 11:12:45 INFO [auto_gptq.quantization.gptq] duration: 5.577987194061279\n",
      "2024-03-04 11:12:45 INFO [auto_gptq.quantization.gptq] avg loss: 0.0010135334450751543\n",
      "INFO - Start quantizing layer 18/32\n",
      "2024-03-04 11:12:46 INFO [auto_gptq.modeling._base] Start quantizing layer 18/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 18/32...\n",
      "2024-03-04 11:12:47 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 18/32...\n",
      "2024-03-04 11:12:49 INFO [auto_gptq.quantization.gptq] duration: 1.7652058601379395\n",
      "2024-03-04 11:12:49 INFO [auto_gptq.quantization.gptq] avg loss: 0.01662958413362503\n",
      "INFO - Quantizing self_attn.v_proj in layer 18/32...\n",
      "2024-03-04 11:12:49 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 18/32...\n",
      "2024-03-04 11:12:51 INFO [auto_gptq.quantization.gptq] duration: 1.5413753986358643\n",
      "2024-03-04 11:12:51 INFO [auto_gptq.quantization.gptq] avg loss: 0.007771835662424564\n",
      "INFO - Quantizing self_attn.q_proj in layer 18/32...\n",
      "2024-03-04 11:12:51 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 18/32...\n",
      "2024-03-04 11:12:52 INFO [auto_gptq.quantization.gptq] duration: 1.5435171127319336\n",
      "2024-03-04 11:12:52 INFO [auto_gptq.quantization.gptq] avg loss: 0.016313375905156136\n",
      "INFO - Quantizing self_attn.o_proj in layer 18/32...\n",
      "2024-03-04 11:12:53 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 18/32...\n",
      "2024-03-04 11:12:54 INFO [auto_gptq.quantization.gptq] duration: 1.6328232288360596\n",
      "2024-03-04 11:12:54 INFO [auto_gptq.quantization.gptq] avg loss: 0.00039761658990755677\n",
      "INFO - Quantizing mlp.up_proj in layer 18/32...\n",
      "2024-03-04 11:12:55 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 18/32...\n",
      "2024-03-04 11:12:57 INFO [auto_gptq.quantization.gptq] duration: 1.8252756595611572\n",
      "2024-03-04 11:12:57 INFO [auto_gptq.quantization.gptq] avg loss: 0.015199754387140274\n",
      "INFO - Quantizing mlp.gate_proj in layer 18/32...\n",
      "2024-03-04 11:12:57 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 18/32...\n",
      "2024-03-04 11:12:59 INFO [auto_gptq.quantization.gptq] duration: 1.7331697940826416\n",
      "2024-03-04 11:12:59 INFO [auto_gptq.quantization.gptq] avg loss: 0.01654457300901413\n",
      "INFO - Quantizing mlp.down_proj in layer 18/32...\n",
      "2024-03-04 11:13:01 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 18/32...\n",
      "2024-03-04 11:13:06 INFO [auto_gptq.quantization.gptq] duration: 5.5731964111328125\n",
      "2024-03-04 11:13:06 INFO [auto_gptq.quantization.gptq] avg loss: 0.0011158990673720837\n",
      "INFO - Start quantizing layer 19/32\n",
      "2024-03-04 11:13:07 INFO [auto_gptq.modeling._base] Start quantizing layer 19/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 19/32...\n",
      "2024-03-04 11:13:09 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 19/32...\n",
      "2024-03-04 11:13:10 INFO [auto_gptq.quantization.gptq] duration: 1.700223684310913\n",
      "2024-03-04 11:13:10 INFO [auto_gptq.quantization.gptq] avg loss: 0.01805151253938675\n",
      "INFO - Quantizing self_attn.v_proj in layer 19/32...\n",
      "2024-03-04 11:13:10 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 19/32...\n",
      "2024-03-04 11:13:12 INFO [auto_gptq.quantization.gptq] duration: 1.5344152450561523\n",
      "2024-03-04 11:13:12 INFO [auto_gptq.quantization.gptq] avg loss: 0.009640417993068695\n",
      "INFO - Quantizing self_attn.q_proj in layer 19/32...\n",
      "2024-03-04 11:13:12 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 19/32...\n",
      "2024-03-04 11:13:13 INFO [auto_gptq.quantization.gptq] duration: 1.5294475555419922\n",
      "2024-03-04 11:13:13 INFO [auto_gptq.quantization.gptq] avg loss: 0.01779744029045105\n",
      "INFO - Quantizing self_attn.o_proj in layer 19/32...\n",
      "2024-03-04 11:13:14 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 19/32...\n",
      "2024-03-04 11:13:16 INFO [auto_gptq.quantization.gptq] duration: 1.636646032333374\n",
      "2024-03-04 11:13:16 INFO [auto_gptq.quantization.gptq] avg loss: 0.0005258377059362829\n",
      "INFO - Quantizing mlp.up_proj in layer 19/32...\n",
      "2024-03-04 11:13:17 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 19/32...\n",
      "2024-03-04 11:13:18 INFO [auto_gptq.quantization.gptq] duration: 1.8389003276824951\n",
      "2024-03-04 11:13:18 INFO [auto_gptq.quantization.gptq] avg loss: 0.016833823174238205\n",
      "INFO - Quantizing mlp.gate_proj in layer 19/32...\n",
      "2024-03-04 11:13:18 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 19/32...\n",
      "2024-03-04 11:13:20 INFO [auto_gptq.quantization.gptq] duration: 1.678391456604004\n",
      "2024-03-04 11:13:20 INFO [auto_gptq.quantization.gptq] avg loss: 0.01876078173518181\n",
      "INFO - Quantizing mlp.down_proj in layer 19/32...\n",
      "2024-03-04 11:13:22 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 19/32...\n",
      "2024-03-04 11:13:28 INFO [auto_gptq.quantization.gptq] duration: 5.571356296539307\n",
      "2024-03-04 11:13:28 INFO [auto_gptq.quantization.gptq] avg loss: 0.001314892084337771\n",
      "INFO - Start quantizing layer 20/32\n",
      "2024-03-04 11:13:29 INFO [auto_gptq.modeling._base] Start quantizing layer 20/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 20/32...\n",
      "2024-03-04 11:13:30 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 20/32...\n",
      "2024-03-04 11:13:32 INFO [auto_gptq.quantization.gptq] duration: 1.7834899425506592\n",
      "2024-03-04 11:13:32 INFO [auto_gptq.quantization.gptq] avg loss: 0.01733185350894928\n",
      "INFO - Quantizing self_attn.v_proj in layer 20/32...\n",
      "2024-03-04 11:13:32 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 20/32...\n",
      "2024-03-04 11:13:33 INFO [auto_gptq.quantization.gptq] duration: 1.5402731895446777\n",
      "2024-03-04 11:13:33 INFO [auto_gptq.quantization.gptq] avg loss: 0.009661270305514336\n",
      "INFO - Quantizing self_attn.q_proj in layer 20/32...\n",
      "2024-03-04 11:13:33 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 20/32...\n",
      "2024-03-04 11:13:35 INFO [auto_gptq.quantization.gptq] duration: 1.540259838104248\n",
      "2024-03-04 11:13:35 INFO [auto_gptq.quantization.gptq] avg loss: 0.017254021018743515\n",
      "INFO - Quantizing self_attn.o_proj in layer 20/32...\n",
      "2024-03-04 11:13:35 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 20/32...\n",
      "2024-03-04 11:13:37 INFO [auto_gptq.quantization.gptq] duration: 1.6561851501464844\n",
      "2024-03-04 11:13:37 INFO [auto_gptq.quantization.gptq] avg loss: 0.0004190538602415472\n",
      "INFO - Quantizing mlp.up_proj in layer 20/32...\n",
      "2024-03-04 11:13:38 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 20/32...\n",
      "2024-03-04 11:13:40 INFO [auto_gptq.quantization.gptq] duration: 1.841939926147461\n",
      "2024-03-04 11:13:40 INFO [auto_gptq.quantization.gptq] avg loss: 0.018200770020484924\n",
      "INFO - Quantizing mlp.gate_proj in layer 20/32...\n",
      "2024-03-04 11:13:40 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 20/32...\n",
      "2024-03-04 11:13:42 INFO [auto_gptq.quantization.gptq] duration: 1.7983338832855225\n",
      "2024-03-04 11:13:42 INFO [auto_gptq.quantization.gptq] avg loss: 0.02045561373233795\n",
      "INFO - Quantizing mlp.down_proj in layer 20/32...\n",
      "2024-03-04 11:13:44 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 20/32...\n",
      "2024-03-04 11:13:49 INFO [auto_gptq.quantization.gptq] duration: 5.623987913131714\n",
      "2024-03-04 11:13:49 INFO [auto_gptq.quantization.gptq] avg loss: 0.001489325426518917\n",
      "INFO - Start quantizing layer 21/32\n",
      "2024-03-04 11:13:50 INFO [auto_gptq.modeling._base] Start quantizing layer 21/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 21/32...\n",
      "2024-03-04 11:13:51 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 21/32...\n",
      "2024-03-04 11:13:53 INFO [auto_gptq.quantization.gptq] duration: 1.695251226425171\n",
      "2024-03-04 11:13:53 INFO [auto_gptq.quantization.gptq] avg loss: 0.01786823198199272\n",
      "INFO - Quantizing self_attn.v_proj in layer 21/32...\n",
      "2024-03-04 11:13:53 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 21/32...\n",
      "2024-03-04 11:13:55 INFO [auto_gptq.quantization.gptq] duration: 1.5325276851654053\n",
      "2024-03-04 11:13:55 INFO [auto_gptq.quantization.gptq] avg loss: 0.010173994116485119\n",
      "INFO - Quantizing self_attn.q_proj in layer 21/32...\n",
      "2024-03-04 11:13:55 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 21/32...\n",
      "2024-03-04 11:13:56 INFO [auto_gptq.quantization.gptq] duration: 1.5284359455108643\n",
      "2024-03-04 11:13:56 INFO [auto_gptq.quantization.gptq] avg loss: 0.01779412478208542\n",
      "INFO - Quantizing self_attn.o_proj in layer 21/32...\n",
      "2024-03-04 11:13:57 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 21/32...\n",
      "2024-03-04 11:13:58 INFO [auto_gptq.quantization.gptq] duration: 1.6466879844665527\n",
      "2024-03-04 11:13:58 INFO [auto_gptq.quantization.gptq] avg loss: 0.00044347118819132447\n",
      "INFO - Quantizing mlp.up_proj in layer 21/32...\n",
      "2024-03-04 11:13:59 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 21/32...\n",
      "2024-03-04 11:14:01 INFO [auto_gptq.quantization.gptq] duration: 1.8327608108520508\n",
      "2024-03-04 11:14:01 INFO [auto_gptq.quantization.gptq] avg loss: 0.018987014889717102\n",
      "INFO - Quantizing mlp.gate_proj in layer 21/32...\n",
      "2024-03-04 11:14:01 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 21/32...\n",
      "2024-03-04 11:14:03 INFO [auto_gptq.quantization.gptq] duration: 1.7146148681640625\n",
      "2024-03-04 11:14:03 INFO [auto_gptq.quantization.gptq] avg loss: 0.021578248590230942\n",
      "INFO - Quantizing mlp.down_proj in layer 21/32...\n",
      "2024-03-04 11:14:05 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 21/32...\n",
      "2024-03-04 11:14:11 INFO [auto_gptq.quantization.gptq] duration: 5.602236270904541\n",
      "2024-03-04 11:14:11 INFO [auto_gptq.quantization.gptq] avg loss: 0.0015958610456436872\n",
      "INFO - Start quantizing layer 22/32\n",
      "2024-03-04 11:14:11 INFO [auto_gptq.modeling._base] Start quantizing layer 22/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 22/32...\n",
      "2024-03-04 11:14:13 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 22/32...\n",
      "2024-03-04 11:14:14 INFO [auto_gptq.quantization.gptq] duration: 1.5669445991516113\n",
      "2024-03-04 11:14:14 INFO [auto_gptq.quantization.gptq] avg loss: 0.018424538895487785\n",
      "INFO - Quantizing self_attn.v_proj in layer 22/32...\n",
      "2024-03-04 11:14:14 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 22/32...\n",
      "2024-03-04 11:14:16 INFO [auto_gptq.quantization.gptq] duration: 1.5394747257232666\n",
      "2024-03-04 11:14:16 INFO [auto_gptq.quantization.gptq] avg loss: 0.011768395081162453\n",
      "INFO - Quantizing self_attn.q_proj in layer 22/32...\n",
      "2024-03-04 11:14:16 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 22/32...\n",
      "2024-03-04 11:14:17 INFO [auto_gptq.quantization.gptq] duration: 1.5384235382080078\n",
      "2024-03-04 11:14:17 INFO [auto_gptq.quantization.gptq] avg loss: 0.018688486889004707\n",
      "INFO - Quantizing self_attn.o_proj in layer 22/32...\n",
      "2024-03-04 11:14:18 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 22/32...\n",
      "2024-03-04 11:14:20 INFO [auto_gptq.quantization.gptq] duration: 1.6371338367462158\n",
      "2024-03-04 11:14:20 INFO [auto_gptq.quantization.gptq] avg loss: 0.000370747409760952\n",
      "INFO - Quantizing mlp.up_proj in layer 22/32...\n",
      "2024-03-04 11:14:21 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 22/32...\n",
      "2024-03-04 11:14:22 INFO [auto_gptq.quantization.gptq] duration: 1.833214521408081\n",
      "2024-03-04 11:14:22 INFO [auto_gptq.quantization.gptq] avg loss: 0.02015889436006546\n",
      "INFO - Quantizing mlp.gate_proj in layer 22/32...\n",
      "2024-03-04 11:14:22 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 22/32...\n",
      "2024-03-04 11:14:24 INFO [auto_gptq.quantization.gptq] duration: 1.7404134273529053\n",
      "2024-03-04 11:14:24 INFO [auto_gptq.quantization.gptq] avg loss: 0.023316847160458565\n",
      "INFO - Quantizing mlp.down_proj in layer 22/32...\n",
      "2024-03-04 11:14:26 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 22/32...\n",
      "2024-03-04 11:14:32 INFO [auto_gptq.quantization.gptq] duration: 5.58159613609314\n",
      "2024-03-04 11:14:32 INFO [auto_gptq.quantization.gptq] avg loss: 0.001672836602665484\n",
      "INFO - Start quantizing layer 23/32\n",
      "2024-03-04 11:14:33 INFO [auto_gptq.modeling._base] Start quantizing layer 23/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 23/32...\n",
      "2024-03-04 11:14:34 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 23/32...\n",
      "2024-03-04 11:14:36 INFO [auto_gptq.quantization.gptq] duration: 1.724557876586914\n",
      "2024-03-04 11:14:36 INFO [auto_gptq.quantization.gptq] avg loss: 0.01984015479683876\n",
      "INFO - Quantizing self_attn.v_proj in layer 23/32...\n",
      "2024-03-04 11:14:36 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 23/32...\n",
      "2024-03-04 11:14:37 INFO [auto_gptq.quantization.gptq] duration: 1.5964910984039307\n",
      "2024-03-04 11:14:37 INFO [auto_gptq.quantization.gptq] avg loss: 0.012164084240794182\n",
      "INFO - Quantizing self_attn.q_proj in layer 23/32...\n",
      "2024-03-04 11:14:37 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 23/32...\n",
      "2024-03-04 11:14:39 INFO [auto_gptq.quantization.gptq] duration: 1.5750889778137207\n",
      "2024-03-04 11:14:39 INFO [auto_gptq.quantization.gptq] avg loss: 0.01991601660847664\n",
      "INFO - Quantizing self_attn.o_proj in layer 23/32...\n",
      "2024-03-04 11:14:40 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 23/32...\n",
      "2024-03-04 11:14:41 INFO [auto_gptq.quantization.gptq] duration: 1.687255859375\n",
      "2024-03-04 11:14:41 INFO [auto_gptq.quantization.gptq] avg loss: 0.0005720843910239637\n",
      "INFO - Quantizing mlp.up_proj in layer 23/32...\n",
      "2024-03-04 11:14:42 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 23/32...\n",
      "2024-03-04 11:14:44 INFO [auto_gptq.quantization.gptq] duration: 1.8244376182556152\n",
      "2024-03-04 11:14:44 INFO [auto_gptq.quantization.gptq] avg loss: 0.020791426301002502\n",
      "INFO - Quantizing mlp.gate_proj in layer 23/32...\n",
      "2024-03-04 11:14:44 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 23/32...\n",
      "2024-03-04 11:14:46 INFO [auto_gptq.quantization.gptq] duration: 1.677018404006958\n",
      "2024-03-04 11:14:46 INFO [auto_gptq.quantization.gptq] avg loss: 0.024367108941078186\n",
      "INFO - Quantizing mlp.down_proj in layer 23/32...\n",
      "2024-03-04 11:14:48 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 23/32...\n",
      "2024-03-04 11:14:53 INFO [auto_gptq.quantization.gptq] duration: 5.715519189834595\n",
      "2024-03-04 11:14:53 INFO [auto_gptq.quantization.gptq] avg loss: 0.001785585656762123\n",
      "INFO - Start quantizing layer 24/32\n",
      "2024-03-04 11:14:54 INFO [auto_gptq.modeling._base] Start quantizing layer 24/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 24/32...\n",
      "2024-03-04 11:14:55 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 24/32...\n",
      "2024-03-04 11:14:57 INFO [auto_gptq.quantization.gptq] duration: 1.6886587142944336\n",
      "2024-03-04 11:14:57 INFO [auto_gptq.quantization.gptq] avg loss: 0.02136831358075142\n",
      "INFO - Quantizing self_attn.v_proj in layer 24/32...\n",
      "2024-03-04 11:14:57 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 24/32...\n",
      "2024-03-04 11:14:59 INFO [auto_gptq.quantization.gptq] duration: 1.5261943340301514\n",
      "2024-03-04 11:14:59 INFO [auto_gptq.quantization.gptq] avg loss: 0.014754441566765308\n",
      "INFO - Quantizing self_attn.q_proj in layer 24/32...\n",
      "2024-03-04 11:14:59 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 24/32...\n",
      "2024-03-04 11:15:00 INFO [auto_gptq.quantization.gptq] duration: 1.5366530418395996\n",
      "2024-03-04 11:15:00 INFO [auto_gptq.quantization.gptq] avg loss: 0.02164604142308235\n",
      "INFO - Quantizing self_attn.o_proj in layer 24/32...\n",
      "2024-03-04 11:15:01 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 24/32...\n",
      "2024-03-04 11:15:03 INFO [auto_gptq.quantization.gptq] duration: 1.6854960918426514\n",
      "2024-03-04 11:15:03 INFO [auto_gptq.quantization.gptq] avg loss: 0.0005509915063157678\n",
      "INFO - Quantizing mlp.up_proj in layer 24/32...\n",
      "2024-03-04 11:15:03 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 24/32...\n",
      "2024-03-04 11:15:05 INFO [auto_gptq.quantization.gptq] duration: 1.8227314949035645\n",
      "2024-03-04 11:15:05 INFO [auto_gptq.quantization.gptq] avg loss: 0.02267567068338394\n",
      "INFO - Quantizing mlp.gate_proj in layer 24/32...\n",
      "2024-03-04 11:15:05 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 24/32...\n",
      "2024-03-04 11:15:07 INFO [auto_gptq.quantization.gptq] duration: 1.6857876777648926\n",
      "2024-03-04 11:15:07 INFO [auto_gptq.quantization.gptq] avg loss: 0.02629275992512703\n",
      "INFO - Quantizing mlp.down_proj in layer 24/32...\n",
      "2024-03-04 11:15:09 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 24/32...\n",
      "2024-03-04 11:15:15 INFO [auto_gptq.quantization.gptq] duration: 5.687669038772583\n",
      "2024-03-04 11:15:15 INFO [auto_gptq.quantization.gptq] avg loss: 0.0019754120148718357\n",
      "INFO - Start quantizing layer 25/32\n",
      "2024-03-04 11:15:16 INFO [auto_gptq.modeling._base] Start quantizing layer 25/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 25/32...\n",
      "2024-03-04 11:15:17 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 25/32...\n",
      "2024-03-04 11:15:18 INFO [auto_gptq.quantization.gptq] duration: 1.6441304683685303\n",
      "2024-03-04 11:15:18 INFO [auto_gptq.quantization.gptq] avg loss: 0.019828403368592262\n",
      "INFO - Quantizing self_attn.v_proj in layer 25/32...\n",
      "2024-03-04 11:15:18 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 25/32...\n",
      "2024-03-04 11:15:20 INFO [auto_gptq.quantization.gptq] duration: 1.5188705921173096\n",
      "2024-03-04 11:15:20 INFO [auto_gptq.quantization.gptq] avg loss: 0.014276750385761261\n",
      "INFO - Quantizing self_attn.q_proj in layer 25/32...\n",
      "2024-03-04 11:15:20 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 25/32...\n",
      "2024-03-04 11:15:22 INFO [auto_gptq.quantization.gptq] duration: 1.5340652465820312\n",
      "2024-03-04 11:15:22 INFO [auto_gptq.quantization.gptq] avg loss: 0.020138511434197426\n",
      "INFO - Quantizing self_attn.o_proj in layer 25/32...\n",
      "2024-03-04 11:15:22 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 25/32...\n",
      "2024-03-04 11:15:24 INFO [auto_gptq.quantization.gptq] duration: 1.6736249923706055\n",
      "2024-03-04 11:15:24 INFO [auto_gptq.quantization.gptq] avg loss: 0.0005534934462048113\n",
      "INFO - Quantizing mlp.up_proj in layer 25/32...\n",
      "2024-03-04 11:15:25 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 25/32...\n",
      "2024-03-04 11:15:27 INFO [auto_gptq.quantization.gptq] duration: 1.822131872177124\n",
      "2024-03-04 11:15:27 INFO [auto_gptq.quantization.gptq] avg loss: 0.023744195699691772\n",
      "INFO - Quantizing mlp.gate_proj in layer 25/32...\n",
      "2024-03-04 11:15:27 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 25/32...\n",
      "2024-03-04 11:15:28 INFO [auto_gptq.quantization.gptq] duration: 1.670029878616333\n",
      "2024-03-04 11:15:28 INFO [auto_gptq.quantization.gptq] avg loss: 0.027514992281794548\n",
      "INFO - Quantizing mlp.down_proj in layer 25/32...\n",
      "2024-03-04 11:15:30 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 25/32...\n",
      "2024-03-04 11:15:36 INFO [auto_gptq.quantization.gptq] duration: 5.66596245765686\n",
      "2024-03-04 11:15:36 INFO [auto_gptq.quantization.gptq] avg loss: 0.00203266111202538\n",
      "INFO - Start quantizing layer 26/32\n",
      "2024-03-04 11:15:37 INFO [auto_gptq.modeling._base] Start quantizing layer 26/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 26/32...\n",
      "2024-03-04 11:15:38 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 26/32...\n",
      "2024-03-04 11:15:40 INFO [auto_gptq.quantization.gptq] duration: 1.6398985385894775\n",
      "2024-03-04 11:15:40 INFO [auto_gptq.quantization.gptq] avg loss: 0.023122375831007957\n",
      "INFO - Quantizing self_attn.v_proj in layer 26/32...\n",
      "2024-03-04 11:15:40 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 26/32...\n",
      "2024-03-04 11:15:41 INFO [auto_gptq.quantization.gptq] duration: 1.5229122638702393\n",
      "2024-03-04 11:15:41 INFO [auto_gptq.quantization.gptq] avg loss: 0.0176718607544899\n",
      "INFO - Quantizing self_attn.q_proj in layer 26/32...\n",
      "2024-03-04 11:15:41 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 26/32...\n",
      "2024-03-04 11:15:43 INFO [auto_gptq.quantization.gptq] duration: 1.5290422439575195\n",
      "2024-03-04 11:15:43 INFO [auto_gptq.quantization.gptq] avg loss: 0.023535333573818207\n",
      "INFO - Quantizing self_attn.o_proj in layer 26/32...\n",
      "2024-03-04 11:15:44 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 26/32...\n",
      "2024-03-04 11:15:45 INFO [auto_gptq.quantization.gptq] duration: 1.6835639476776123\n",
      "2024-03-04 11:15:45 INFO [auto_gptq.quantization.gptq] avg loss: 0.00039524963358417153\n",
      "INFO - Quantizing mlp.up_proj in layer 26/32...\n",
      "2024-03-04 11:15:46 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 26/32...\n",
      "2024-03-04 11:15:48 INFO [auto_gptq.quantization.gptq] duration: 1.8504788875579834\n",
      "2024-03-04 11:15:48 INFO [auto_gptq.quantization.gptq] avg loss: 0.025699231773614883\n",
      "INFO - Quantizing mlp.gate_proj in layer 26/32...\n",
      "2024-03-04 11:15:48 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 26/32...\n",
      "2024-03-04 11:15:50 INFO [auto_gptq.quantization.gptq] duration: 1.6840922832489014\n",
      "2024-03-04 11:15:50 INFO [auto_gptq.quantization.gptq] avg loss: 0.029664430767297745\n",
      "INFO - Quantizing mlp.down_proj in layer 26/32...\n",
      "2024-03-04 11:15:52 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 26/32...\n",
      "2024-03-04 11:15:57 INFO [auto_gptq.quantization.gptq] duration: 5.731983184814453\n",
      "2024-03-04 11:15:57 INFO [auto_gptq.quantization.gptq] avg loss: 0.0022467495873570442\n",
      "INFO - Start quantizing layer 27/32\n",
      "2024-03-04 11:15:58 INFO [auto_gptq.modeling._base] Start quantizing layer 27/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 27/32...\n",
      "2024-03-04 11:16:00 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 27/32...\n",
      "2024-03-04 11:16:01 INFO [auto_gptq.quantization.gptq] duration: 1.7059540748596191\n",
      "2024-03-04 11:16:01 INFO [auto_gptq.quantization.gptq] avg loss: 0.02174369990825653\n",
      "INFO - Quantizing self_attn.v_proj in layer 27/32...\n",
      "2024-03-04 11:16:01 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 27/32...\n",
      "2024-03-04 11:16:03 INFO [auto_gptq.quantization.gptq] duration: 1.545907735824585\n",
      "2024-03-04 11:16:03 INFO [auto_gptq.quantization.gptq] avg loss: 0.017507381737232208\n",
      "INFO - Quantizing self_attn.q_proj in layer 27/32...\n",
      "2024-03-04 11:16:03 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 27/32...\n",
      "2024-03-04 11:16:04 INFO [auto_gptq.quantization.gptq] duration: 1.553882360458374\n",
      "2024-03-04 11:16:04 INFO [auto_gptq.quantization.gptq] avg loss: 0.02229401282966137\n",
      "INFO - Quantizing self_attn.o_proj in layer 27/32...\n",
      "2024-03-04 11:16:05 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 27/32...\n",
      "2024-03-04 11:16:07 INFO [auto_gptq.quantization.gptq] duration: 1.6985502243041992\n",
      "2024-03-04 11:16:07 INFO [auto_gptq.quantization.gptq] avg loss: 0.0008275508880615234\n",
      "INFO - Quantizing mlp.up_proj in layer 27/32...\n",
      "2024-03-04 11:16:08 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 27/32...\n",
      "2024-03-04 11:16:09 INFO [auto_gptq.quantization.gptq] duration: 1.849428653717041\n",
      "2024-03-04 11:16:09 INFO [auto_gptq.quantization.gptq] avg loss: 0.02722514420747757\n",
      "INFO - Quantizing mlp.gate_proj in layer 27/32...\n",
      "2024-03-04 11:16:09 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 27/32...\n",
      "2024-03-04 11:16:11 INFO [auto_gptq.quantization.gptq] duration: 1.6753180027008057\n",
      "2024-03-04 11:16:11 INFO [auto_gptq.quantization.gptq] avg loss: 0.03138182312250137\n",
      "INFO - Quantizing mlp.down_proj in layer 27/32...\n",
      "2024-03-04 11:16:13 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 27/32...\n",
      "2024-03-04 11:16:19 INFO [auto_gptq.quantization.gptq] duration: 5.702543497085571\n",
      "2024-03-04 11:16:19 INFO [auto_gptq.quantization.gptq] avg loss: 0.0024676916655153036\n",
      "INFO - Start quantizing layer 28/32\n",
      "2024-03-04 11:16:20 INFO [auto_gptq.modeling._base] Start quantizing layer 28/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 28/32...\n",
      "2024-03-04 11:16:21 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 28/32...\n",
      "2024-03-04 11:16:23 INFO [auto_gptq.quantization.gptq] duration: 1.6891047954559326\n",
      "2024-03-04 11:16:23 INFO [auto_gptq.quantization.gptq] avg loss: 0.024259362369775772\n",
      "INFO - Quantizing self_attn.v_proj in layer 28/32...\n",
      "2024-03-04 11:16:23 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 28/32...\n",
      "2024-03-04 11:16:24 INFO [auto_gptq.quantization.gptq] duration: 1.5155835151672363\n",
      "2024-03-04 11:16:24 INFO [auto_gptq.quantization.gptq] avg loss: 0.018333308398723602\n",
      "INFO - Quantizing self_attn.q_proj in layer 28/32...\n",
      "2024-03-04 11:16:24 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 28/32...\n",
      "2024-03-04 11:16:26 INFO [auto_gptq.quantization.gptq] duration: 1.5259699821472168\n",
      "2024-03-04 11:16:26 INFO [auto_gptq.quantization.gptq] avg loss: 0.024768808856606483\n",
      "INFO - Quantizing self_attn.o_proj in layer 28/32...\n",
      "2024-03-04 11:16:26 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 28/32...\n",
      "2024-03-04 11:16:28 INFO [auto_gptq.quantization.gptq] duration: 1.6721434593200684\n",
      "2024-03-04 11:16:28 INFO [auto_gptq.quantization.gptq] avg loss: 0.0007987311691977084\n",
      "INFO - Quantizing mlp.up_proj in layer 28/32...\n",
      "2024-03-04 11:16:29 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 28/32...\n",
      "2024-03-04 11:16:31 INFO [auto_gptq.quantization.gptq] duration: 1.8224880695343018\n",
      "2024-03-04 11:16:31 INFO [auto_gptq.quantization.gptq] avg loss: 0.02928605116903782\n",
      "INFO - Quantizing mlp.gate_proj in layer 28/32...\n",
      "2024-03-04 11:16:31 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 28/32...\n",
      "2024-03-04 11:16:32 INFO [auto_gptq.quantization.gptq] duration: 1.67698073387146\n",
      "2024-03-04 11:16:32 INFO [auto_gptq.quantization.gptq] avg loss: 0.0334881991147995\n",
      "INFO - Quantizing mlp.down_proj in layer 28/32...\n",
      "2024-03-04 11:16:34 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 28/32...\n",
      "2024-03-04 11:16:40 INFO [auto_gptq.quantization.gptq] duration: 5.691099643707275\n",
      "2024-03-04 11:16:40 INFO [auto_gptq.quantization.gptq] avg loss: 0.0028429394587874413\n",
      "INFO - Start quantizing layer 29/32\n",
      "2024-03-04 11:16:41 INFO [auto_gptq.modeling._base] Start quantizing layer 29/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 29/32...\n",
      "2024-03-04 11:16:42 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 29/32...\n",
      "2024-03-04 11:16:44 INFO [auto_gptq.quantization.gptq] duration: 1.670626163482666\n",
      "2024-03-04 11:16:44 INFO [auto_gptq.quantization.gptq] avg loss: 0.02383701130747795\n",
      "INFO - Quantizing self_attn.v_proj in layer 29/32...\n",
      "2024-03-04 11:16:44 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 29/32...\n",
      "2024-03-04 11:16:45 INFO [auto_gptq.quantization.gptq] duration: 1.52583909034729\n",
      "2024-03-04 11:16:45 INFO [auto_gptq.quantization.gptq] avg loss: 0.020187806338071823\n",
      "INFO - Quantizing self_attn.q_proj in layer 29/32...\n",
      "2024-03-04 11:16:45 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 29/32...\n",
      "2024-03-04 11:16:47 INFO [auto_gptq.quantization.gptq] duration: 1.5314075946807861\n",
      "2024-03-04 11:16:47 INFO [auto_gptq.quantization.gptq] avg loss: 0.0245867557823658\n",
      "INFO - Quantizing self_attn.o_proj in layer 29/32...\n",
      "2024-03-04 11:16:48 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 29/32...\n",
      "2024-03-04 11:16:49 INFO [auto_gptq.quantization.gptq] duration: 1.68135666847229\n",
      "2024-03-04 11:16:49 INFO [auto_gptq.quantization.gptq] avg loss: 0.0009660296491347253\n",
      "INFO - Quantizing mlp.up_proj in layer 29/32...\n",
      "2024-03-04 11:16:50 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 29/32...\n",
      "2024-03-04 11:16:52 INFO [auto_gptq.quantization.gptq] duration: 1.8233072757720947\n",
      "2024-03-04 11:16:52 INFO [auto_gptq.quantization.gptq] avg loss: 0.031107019633054733\n",
      "INFO - Quantizing mlp.gate_proj in layer 29/32...\n",
      "2024-03-04 11:16:52 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 29/32...\n",
      "2024-03-04 11:16:54 INFO [auto_gptq.quantization.gptq] duration: 1.6680703163146973\n",
      "2024-03-04 11:16:54 INFO [auto_gptq.quantization.gptq] avg loss: 0.03453921899199486\n",
      "INFO - Quantizing mlp.down_proj in layer 29/32...\n",
      "2024-03-04 11:16:56 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 29/32...\n",
      "2024-03-04 11:17:02 INFO [auto_gptq.quantization.gptq] duration: 5.70731520652771\n",
      "2024-03-04 11:17:02 INFO [auto_gptq.quantization.gptq] avg loss: 0.0033529934007674456\n",
      "INFO - Start quantizing layer 30/32\n",
      "2024-03-04 11:17:02 INFO [auto_gptq.modeling._base] Start quantizing layer 30/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 30/32...\n",
      "2024-03-04 11:17:04 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 30/32...\n",
      "2024-03-04 11:17:05 INFO [auto_gptq.quantization.gptq] duration: 1.688091516494751\n",
      "2024-03-04 11:17:05 INFO [auto_gptq.quantization.gptq] avg loss: 0.020734744146466255\n",
      "INFO - Quantizing self_attn.v_proj in layer 30/32...\n",
      "2024-03-04 11:17:05 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 30/32...\n",
      "2024-03-04 11:17:07 INFO [auto_gptq.quantization.gptq] duration: 1.520993709564209\n",
      "2024-03-04 11:17:07 INFO [auto_gptq.quantization.gptq] avg loss: 0.018744943663477898\n",
      "INFO - Quantizing self_attn.q_proj in layer 30/32...\n",
      "2024-03-04 11:17:07 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 30/32...\n",
      "2024-03-04 11:17:08 INFO [auto_gptq.quantization.gptq] duration: 1.5207922458648682\n",
      "2024-03-04 11:17:08 INFO [auto_gptq.quantization.gptq] avg loss: 0.022411951795220375\n",
      "INFO - Quantizing self_attn.o_proj in layer 30/32...\n",
      "2024-03-04 11:17:09 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 30/32...\n",
      "2024-03-04 11:17:11 INFO [auto_gptq.quantization.gptq] duration: 1.6821517944335938\n",
      "2024-03-04 11:17:11 INFO [auto_gptq.quantization.gptq] avg loss: 0.0007727569900453091\n",
      "INFO - Quantizing mlp.up_proj in layer 30/32...\n",
      "2024-03-04 11:17:12 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 30/32...\n",
      "2024-03-04 11:17:13 INFO [auto_gptq.quantization.gptq] duration: 1.8262865543365479\n",
      "2024-03-04 11:17:13 INFO [auto_gptq.quantization.gptq] avg loss: 0.03247595205903053\n",
      "INFO - Quantizing mlp.gate_proj in layer 30/32...\n",
      "2024-03-04 11:17:13 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 30/32...\n",
      "2024-03-04 11:17:15 INFO [auto_gptq.quantization.gptq] duration: 1.674788475036621\n",
      "2024-03-04 11:17:15 INFO [auto_gptq.quantization.gptq] avg loss: 0.03563118726015091\n",
      "INFO - Quantizing mlp.down_proj in layer 30/32...\n",
      "2024-03-04 11:17:17 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 30/32...\n",
      "2024-03-04 11:17:23 INFO [auto_gptq.quantization.gptq] duration: 6.021866798400879\n",
      "2024-03-04 11:17:23 INFO [auto_gptq.quantization.gptq] avg loss: 0.004226570017635822\n",
      "INFO - Start quantizing layer 31/32\n",
      "2024-03-04 11:17:24 INFO [auto_gptq.modeling._base] Start quantizing layer 31/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 31/32...\n",
      "2024-03-04 11:17:25 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 31/32...\n",
      "2024-03-04 11:17:27 INFO [auto_gptq.quantization.gptq] duration: 1.6895325183868408\n",
      "2024-03-04 11:17:27 INFO [auto_gptq.quantization.gptq] avg loss: 0.022517675533890724\n",
      "INFO - Quantizing self_attn.v_proj in layer 31/32...\n",
      "2024-03-04 11:17:27 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 31/32...\n",
      "2024-03-04 11:17:29 INFO [auto_gptq.quantization.gptq] duration: 1.5275685787200928\n",
      "2024-03-04 11:17:29 INFO [auto_gptq.quantization.gptq] avg loss: 0.02085883542895317\n",
      "INFO - Quantizing self_attn.q_proj in layer 31/32...\n",
      "2024-03-04 11:17:29 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 31/32...\n",
      "2024-03-04 11:17:30 INFO [auto_gptq.quantization.gptq] duration: 1.5264194011688232\n",
      "2024-03-04 11:17:30 INFO [auto_gptq.quantization.gptq] avg loss: 0.023569002747535706\n",
      "INFO - Quantizing self_attn.o_proj in layer 31/32...\n",
      "2024-03-04 11:17:31 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 31/32...\n",
      "2024-03-04 11:17:32 INFO [auto_gptq.quantization.gptq] duration: 1.6397802829742432\n",
      "2024-03-04 11:17:32 INFO [auto_gptq.quantization.gptq] avg loss: 0.0011048618471249938\n",
      "INFO - Quantizing mlp.up_proj in layer 31/32...\n",
      "2024-03-04 11:17:33 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 31/32...\n",
      "2024-03-04 11:17:35 INFO [auto_gptq.quantization.gptq] duration: 1.9003181457519531\n",
      "2024-03-04 11:17:35 INFO [auto_gptq.quantization.gptq] avg loss: 0.032899294048547745\n",
      "INFO - Quantizing mlp.gate_proj in layer 31/32...\n",
      "2024-03-04 11:17:35 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 31/32...\n",
      "2024-03-04 11:17:37 INFO [auto_gptq.quantization.gptq] duration: 1.6754555702209473\n",
      "2024-03-04 11:17:37 INFO [auto_gptq.quantization.gptq] avg loss: 0.03661322593688965\n",
      "INFO - Quantizing mlp.down_proj in layer 31/32...\n",
      "2024-03-04 11:17:39 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 31/32...\n",
      "2024-03-04 11:17:45 INFO [auto_gptq.quantization.gptq] duration: 5.579615831375122\n",
      "2024-03-04 11:17:45 INFO [auto_gptq.quantization.gptq] avg loss: 0.028545863926410675\n",
      "INFO - Start quantizing layer 32/32\n",
      "2024-03-04 11:17:45 INFO [auto_gptq.modeling._base] Start quantizing layer 32/32\n",
      "INFO - Quantizing self_attn.k_proj in layer 32/32...\n",
      "2024-03-04 11:17:47 INFO [auto_gptq.modeling._base] Quantizing self_attn.k_proj in layer 32/32...\n",
      "2024-03-04 11:17:49 INFO [auto_gptq.quantization.gptq] duration: 1.7988450527191162\n",
      "2024-03-04 11:17:49 INFO [auto_gptq.quantization.gptq] avg loss: 0.016238104552030563\n",
      "INFO - Quantizing self_attn.v_proj in layer 32/32...\n",
      "2024-03-04 11:17:49 INFO [auto_gptq.modeling._base] Quantizing self_attn.v_proj in layer 32/32...\n",
      "2024-03-04 11:17:50 INFO [auto_gptq.quantization.gptq] duration: 1.520592451095581\n",
      "2024-03-04 11:17:50 INFO [auto_gptq.quantization.gptq] avg loss: 0.011609284207224846\n",
      "INFO - Quantizing self_attn.q_proj in layer 32/32...\n",
      "2024-03-04 11:17:50 INFO [auto_gptq.modeling._base] Quantizing self_attn.q_proj in layer 32/32...\n",
      "2024-03-04 11:17:52 INFO [auto_gptq.quantization.gptq] duration: 1.5246877670288086\n",
      "2024-03-04 11:17:52 INFO [auto_gptq.quantization.gptq] avg loss: 0.015594258904457092\n",
      "INFO - Quantizing self_attn.o_proj in layer 32/32...\n",
      "2024-03-04 11:17:52 INFO [auto_gptq.modeling._base] Quantizing self_attn.o_proj in layer 32/32...\n",
      "2024-03-04 11:17:54 INFO [auto_gptq.quantization.gptq] duration: 1.6355507373809814\n",
      "2024-03-04 11:17:54 INFO [auto_gptq.quantization.gptq] avg loss: 0.0014409518335014582\n",
      "INFO - Quantizing mlp.up_proj in layer 32/32...\n",
      "2024-03-04 11:17:55 INFO [auto_gptq.modeling._base] Quantizing mlp.up_proj in layer 32/32...\n",
      "2024-03-04 11:17:57 INFO [auto_gptq.quantization.gptq] duration: 1.8221454620361328\n",
      "2024-03-04 11:17:57 INFO [auto_gptq.quantization.gptq] avg loss: 0.028229864314198494\n",
      "INFO - Quantizing mlp.gate_proj in layer 32/32...\n",
      "2024-03-04 11:17:57 INFO [auto_gptq.modeling._base] Quantizing mlp.gate_proj in layer 32/32...\n",
      "2024-03-04 11:17:58 INFO [auto_gptq.quantization.gptq] duration: 1.6600329875946045\n",
      "2024-03-04 11:17:58 INFO [auto_gptq.quantization.gptq] avg loss: 0.03165965527296066\n",
      "INFO - Quantizing mlp.down_proj in layer 32/32...\n",
      "2024-03-04 11:18:00 INFO [auto_gptq.modeling._base] Quantizing mlp.down_proj in layer 32/32...\n",
      "2024-03-04 11:18:06 INFO [auto_gptq.quantization.gptq] duration: 5.5707948207855225\n",
      "2024-03-04 11:18:06 INFO [auto_gptq.quantization.gptq] avg loss: 0.017476774752140045\n",
      "2024-03-04 11:18:07 INFO [auto_gptq.modeling._utils] Packing model...\n",
      "2024-03-04 11:18:08 INFO [auto_gptq.modeling._utils] model.layers.0.self_attn.q_proj\n",
      "2024-03-04 11:18:09 INFO [auto_gptq.modeling._utils] model.layers.0.self_attn.k_proj\n",
      "2024-03-04 11:18:10 INFO [auto_gptq.modeling._utils] model.layers.0.self_attn.v_proj\n",
      "2024-03-04 11:18:11 INFO [auto_gptq.modeling._utils] model.layers.0.self_attn.o_proj\n",
      "2024-03-04 11:18:12 INFO [auto_gptq.modeling._utils] model.layers.0.mlp.gate_proj\n",
      "2024-03-04 11:18:15 INFO [auto_gptq.modeling._utils] model.layers.0.mlp.up_proj\n",
      "2024-03-04 11:18:17 INFO [auto_gptq.modeling._utils] model.layers.0.mlp.down_proj\n",
      "2024-03-04 11:18:20 INFO [auto_gptq.modeling._utils] model.layers.1.self_attn.q_proj\n",
      "2024-03-04 11:18:21 INFO [auto_gptq.modeling._utils] model.layers.1.self_attn.k_proj\n",
      "2024-03-04 11:18:22 INFO [auto_gptq.modeling._utils] model.layers.1.self_attn.v_proj\n",
      "2024-03-04 11:18:23 INFO [auto_gptq.modeling._utils] model.layers.1.self_attn.o_proj\n",
      "2024-03-04 11:18:24 INFO [auto_gptq.modeling._utils] model.layers.1.mlp.gate_proj\n",
      "2024-03-04 11:18:26 INFO [auto_gptq.modeling._utils] model.layers.1.mlp.up_proj\n",
      "2024-03-04 11:18:29 INFO [auto_gptq.modeling._utils] model.layers.1.mlp.down_proj\n",
      "2024-03-04 11:18:32 INFO [auto_gptq.modeling._utils] model.layers.2.self_attn.q_proj\n",
      "2024-03-04 11:18:33 INFO [auto_gptq.modeling._utils] model.layers.2.self_attn.k_proj\n",
      "2024-03-04 11:18:34 INFO [auto_gptq.modeling._utils] model.layers.2.self_attn.v_proj\n",
      "2024-03-04 11:18:35 INFO [auto_gptq.modeling._utils] model.layers.2.self_attn.o_proj\n",
      "2024-03-04 11:18:36 INFO [auto_gptq.modeling._utils] model.layers.2.mlp.gate_proj\n",
      "2024-03-04 11:18:39 INFO [auto_gptq.modeling._utils] model.layers.2.mlp.up_proj\n",
      "2024-03-04 11:18:42 INFO [auto_gptq.modeling._utils] model.layers.2.mlp.down_proj\n",
      "2024-03-04 11:18:44 INFO [auto_gptq.modeling._utils] model.layers.3.self_attn.q_proj\n",
      "2024-03-04 11:18:45 INFO [auto_gptq.modeling._utils] model.layers.3.self_attn.k_proj\n",
      "2024-03-04 11:18:46 INFO [auto_gptq.modeling._utils] model.layers.3.self_attn.v_proj\n",
      "2024-03-04 11:18:47 INFO [auto_gptq.modeling._utils] model.layers.3.self_attn.o_proj\n",
      "2024-03-04 11:18:48 INFO [auto_gptq.modeling._utils] model.layers.3.mlp.gate_proj\n",
      "2024-03-04 11:18:51 INFO [auto_gptq.modeling._utils] model.layers.3.mlp.up_proj\n",
      "2024-03-04 11:18:53 INFO [auto_gptq.modeling._utils] model.layers.3.mlp.down_proj\n",
      "2024-03-04 11:18:56 INFO [auto_gptq.modeling._utils] model.layers.4.self_attn.q_proj\n",
      "2024-03-04 11:18:57 INFO [auto_gptq.modeling._utils] model.layers.4.self_attn.k_proj\n",
      "2024-03-04 11:18:58 INFO [auto_gptq.modeling._utils] model.layers.4.self_attn.v_proj\n",
      "2024-03-04 11:18:59 INFO [auto_gptq.modeling._utils] model.layers.4.self_attn.o_proj\n",
      "2024-03-04 11:19:00 INFO [auto_gptq.modeling._utils] model.layers.4.mlp.gate_proj\n",
      "2024-03-04 11:19:03 INFO [auto_gptq.modeling._utils] model.layers.4.mlp.up_proj\n",
      "2024-03-04 11:19:05 INFO [auto_gptq.modeling._utils] model.layers.4.mlp.down_proj\n",
      "2024-03-04 11:19:08 INFO [auto_gptq.modeling._utils] model.layers.5.self_attn.q_proj\n",
      "2024-03-04 11:19:09 INFO [auto_gptq.modeling._utils] model.layers.5.self_attn.k_proj\n",
      "2024-03-04 11:19:10 INFO [auto_gptq.modeling._utils] model.layers.5.self_attn.v_proj\n",
      "2024-03-04 11:19:11 INFO [auto_gptq.modeling._utils] model.layers.5.self_attn.o_proj\n",
      "2024-03-04 11:19:12 INFO [auto_gptq.modeling._utils] model.layers.5.mlp.gate_proj\n",
      "2024-03-04 11:19:15 INFO [auto_gptq.modeling._utils] model.layers.5.mlp.up_proj\n",
      "2024-03-04 11:19:17 INFO [auto_gptq.modeling._utils] model.layers.5.mlp.down_proj\n",
      "2024-03-04 11:19:20 INFO [auto_gptq.modeling._utils] model.layers.6.self_attn.q_proj\n",
      "2024-03-04 11:19:21 INFO [auto_gptq.modeling._utils] model.layers.6.self_attn.k_proj\n",
      "2024-03-04 11:19:22 INFO [auto_gptq.modeling._utils] model.layers.6.self_attn.v_proj\n",
      "2024-03-04 11:19:23 INFO [auto_gptq.modeling._utils] model.layers.6.self_attn.o_proj\n",
      "2024-03-04 11:19:24 INFO [auto_gptq.modeling._utils] model.layers.6.mlp.gate_proj\n",
      "2024-03-04 11:19:27 INFO [auto_gptq.modeling._utils] model.layers.6.mlp.up_proj\n",
      "2024-03-04 11:19:29 INFO [auto_gptq.modeling._utils] model.layers.6.mlp.down_proj\n",
      "2024-03-04 11:19:32 INFO [auto_gptq.modeling._utils] model.layers.7.self_attn.q_proj\n",
      "2024-03-04 11:19:33 INFO [auto_gptq.modeling._utils] model.layers.7.self_attn.k_proj\n",
      "2024-03-04 11:19:34 INFO [auto_gptq.modeling._utils] model.layers.7.self_attn.v_proj\n",
      "2024-03-04 11:19:35 INFO [auto_gptq.modeling._utils] model.layers.7.self_attn.o_proj\n",
      "2024-03-04 11:19:36 INFO [auto_gptq.modeling._utils] model.layers.7.mlp.gate_proj\n",
      "2024-03-04 11:19:38 INFO [auto_gptq.modeling._utils] model.layers.7.mlp.up_proj\n",
      "2024-03-04 11:19:41 INFO [auto_gptq.modeling._utils] model.layers.7.mlp.down_proj\n",
      "2024-03-04 11:19:44 INFO [auto_gptq.modeling._utils] model.layers.8.self_attn.q_proj\n",
      "2024-03-04 11:19:45 INFO [auto_gptq.modeling._utils] model.layers.8.self_attn.k_proj\n",
      "2024-03-04 11:19:46 INFO [auto_gptq.modeling._utils] model.layers.8.self_attn.v_proj\n",
      "2024-03-04 11:19:46 INFO [auto_gptq.modeling._utils] model.layers.8.self_attn.o_proj\n",
      "2024-03-04 11:19:47 INFO [auto_gptq.modeling._utils] model.layers.8.mlp.gate_proj\n",
      "2024-03-04 11:19:50 INFO [auto_gptq.modeling._utils] model.layers.8.mlp.up_proj\n",
      "2024-03-04 11:19:53 INFO [auto_gptq.modeling._utils] model.layers.8.mlp.down_proj\n",
      "2024-03-04 11:19:55 INFO [auto_gptq.modeling._utils] model.layers.9.self_attn.q_proj\n",
      "2024-03-04 11:19:56 INFO [auto_gptq.modeling._utils] model.layers.9.self_attn.k_proj\n",
      "2024-03-04 11:19:57 INFO [auto_gptq.modeling._utils] model.layers.9.self_attn.v_proj\n",
      "2024-03-04 11:19:58 INFO [auto_gptq.modeling._utils] model.layers.9.self_attn.o_proj\n",
      "2024-03-04 11:19:59 INFO [auto_gptq.modeling._utils] model.layers.9.mlp.gate_proj\n",
      "2024-03-04 11:20:02 INFO [auto_gptq.modeling._utils] model.layers.9.mlp.up_proj\n",
      "2024-03-04 11:20:05 INFO [auto_gptq.modeling._utils] model.layers.9.mlp.down_proj\n",
      "2024-03-04 11:20:07 INFO [auto_gptq.modeling._utils] model.layers.10.self_attn.q_proj\n",
      "2024-03-04 11:20:08 INFO [auto_gptq.modeling._utils] model.layers.10.self_attn.k_proj\n",
      "2024-03-04 11:20:09 INFO [auto_gptq.modeling._utils] model.layers.10.self_attn.v_proj\n",
      "2024-03-04 11:20:10 INFO [auto_gptq.modeling._utils] model.layers.10.self_attn.o_proj\n",
      "2024-03-04 11:20:11 INFO [auto_gptq.modeling._utils] model.layers.10.mlp.gate_proj\n",
      "2024-03-04 11:20:14 INFO [auto_gptq.modeling._utils] model.layers.10.mlp.up_proj\n",
      "2024-03-04 11:20:17 INFO [auto_gptq.modeling._utils] model.layers.10.mlp.down_proj\n",
      "2024-03-04 11:20:19 INFO [auto_gptq.modeling._utils] model.layers.11.self_attn.q_proj\n",
      "2024-03-04 11:20:20 INFO [auto_gptq.modeling._utils] model.layers.11.self_attn.k_proj\n",
      "2024-03-04 11:20:21 INFO [auto_gptq.modeling._utils] model.layers.11.self_attn.v_proj\n",
      "2024-03-04 11:20:22 INFO [auto_gptq.modeling._utils] model.layers.11.self_attn.o_proj\n",
      "2024-03-04 11:20:23 INFO [auto_gptq.modeling._utils] model.layers.11.mlp.gate_proj\n",
      "2024-03-04 11:20:26 INFO [auto_gptq.modeling._utils] model.layers.11.mlp.up_proj\n",
      "2024-03-04 11:20:28 INFO [auto_gptq.modeling._utils] model.layers.11.mlp.down_proj\n",
      "2024-03-04 11:20:31 INFO [auto_gptq.modeling._utils] model.layers.12.self_attn.q_proj\n",
      "2024-03-04 11:20:32 INFO [auto_gptq.modeling._utils] model.layers.12.self_attn.k_proj\n",
      "2024-03-04 11:20:33 INFO [auto_gptq.modeling._utils] model.layers.12.self_attn.v_proj\n",
      "2024-03-04 11:20:34 INFO [auto_gptq.modeling._utils] model.layers.12.self_attn.o_proj\n",
      "2024-03-04 11:20:35 INFO [auto_gptq.modeling._utils] model.layers.12.mlp.gate_proj\n",
      "2024-03-04 11:20:38 INFO [auto_gptq.modeling._utils] model.layers.12.mlp.up_proj\n",
      "2024-03-04 11:20:41 INFO [auto_gptq.modeling._utils] model.layers.12.mlp.down_proj\n",
      "2024-03-04 11:20:44 INFO [auto_gptq.modeling._utils] model.layers.13.self_attn.q_proj\n",
      "2024-03-04 11:20:45 INFO [auto_gptq.modeling._utils] model.layers.13.self_attn.k_proj\n",
      "2024-03-04 11:20:46 INFO [auto_gptq.modeling._utils] model.layers.13.self_attn.v_proj\n",
      "2024-03-04 11:20:47 INFO [auto_gptq.modeling._utils] model.layers.13.self_attn.o_proj\n",
      "2024-03-04 11:20:48 INFO [auto_gptq.modeling._utils] model.layers.13.mlp.gate_proj\n",
      "2024-03-04 11:20:51 INFO [auto_gptq.modeling._utils] model.layers.13.mlp.up_proj\n",
      "2024-03-04 11:20:54 INFO [auto_gptq.modeling._utils] model.layers.13.mlp.down_proj\n",
      "2024-03-04 11:20:56 INFO [auto_gptq.modeling._utils] model.layers.14.self_attn.q_proj\n",
      "2024-03-04 11:20:58 INFO [auto_gptq.modeling._utils] model.layers.14.self_attn.k_proj\n",
      "2024-03-04 11:20:59 INFO [auto_gptq.modeling._utils] model.layers.14.self_attn.v_proj\n",
      "2024-03-04 11:21:00 INFO [auto_gptq.modeling._utils] model.layers.14.self_attn.o_proj\n",
      "2024-03-04 11:21:01 INFO [auto_gptq.modeling._utils] model.layers.14.mlp.gate_proj\n",
      "2024-03-04 11:21:04 INFO [auto_gptq.modeling._utils] model.layers.14.mlp.up_proj\n",
      "2024-03-04 11:21:06 INFO [auto_gptq.modeling._utils] model.layers.14.mlp.down_proj\n",
      "2024-03-04 11:21:09 INFO [auto_gptq.modeling._utils] model.layers.15.self_attn.q_proj\n",
      "2024-03-04 11:21:10 INFO [auto_gptq.modeling._utils] model.layers.15.self_attn.k_proj\n",
      "2024-03-04 11:21:11 INFO [auto_gptq.modeling._utils] model.layers.15.self_attn.v_proj\n",
      "2024-03-04 11:21:12 INFO [auto_gptq.modeling._utils] model.layers.15.self_attn.o_proj\n",
      "2024-03-04 11:21:13 INFO [auto_gptq.modeling._utils] model.layers.15.mlp.gate_proj\n",
      "2024-03-04 11:21:15 INFO [auto_gptq.modeling._utils] model.layers.15.mlp.up_proj\n",
      "2024-03-04 11:21:18 INFO [auto_gptq.modeling._utils] model.layers.15.mlp.down_proj\n",
      "2024-03-04 11:21:20 INFO [auto_gptq.modeling._utils] model.layers.16.self_attn.q_proj\n",
      "2024-03-04 11:21:21 INFO [auto_gptq.modeling._utils] model.layers.16.self_attn.k_proj\n",
      "2024-03-04 11:21:22 INFO [auto_gptq.modeling._utils] model.layers.16.self_attn.v_proj\n",
      "2024-03-04 11:21:23 INFO [auto_gptq.modeling._utils] model.layers.16.self_attn.o_proj\n",
      "2024-03-04 11:21:24 INFO [auto_gptq.modeling._utils] model.layers.16.mlp.gate_proj\n",
      "2024-03-04 11:21:27 INFO [auto_gptq.modeling._utils] model.layers.16.mlp.up_proj\n",
      "2024-03-04 11:21:29 INFO [auto_gptq.modeling._utils] model.layers.16.mlp.down_proj\n",
      "2024-03-04 11:21:32 INFO [auto_gptq.modeling._utils] model.layers.17.self_attn.q_proj\n",
      "2024-03-04 11:21:33 INFO [auto_gptq.modeling._utils] model.layers.17.self_attn.k_proj\n",
      "2024-03-04 11:21:34 INFO [auto_gptq.modeling._utils] model.layers.17.self_attn.v_proj\n",
      "2024-03-04 11:21:34 INFO [auto_gptq.modeling._utils] model.layers.17.self_attn.o_proj\n",
      "2024-03-04 11:21:35 INFO [auto_gptq.modeling._utils] model.layers.17.mlp.gate_proj\n",
      "2024-03-04 11:21:38 INFO [auto_gptq.modeling._utils] model.layers.17.mlp.up_proj\n",
      "2024-03-04 11:21:41 INFO [auto_gptq.modeling._utils] model.layers.17.mlp.down_proj\n",
      "2024-03-04 11:21:43 INFO [auto_gptq.modeling._utils] model.layers.18.self_attn.q_proj\n",
      "2024-03-04 11:21:44 INFO [auto_gptq.modeling._utils] model.layers.18.self_attn.k_proj\n",
      "2024-03-04 11:21:45 INFO [auto_gptq.modeling._utils] model.layers.18.self_attn.v_proj\n",
      "2024-03-04 11:21:46 INFO [auto_gptq.modeling._utils] model.layers.18.self_attn.o_proj\n",
      "2024-03-04 11:21:47 INFO [auto_gptq.modeling._utils] model.layers.18.mlp.gate_proj\n",
      "2024-03-04 11:21:50 INFO [auto_gptq.modeling._utils] model.layers.18.mlp.up_proj\n",
      "2024-03-04 11:21:52 INFO [auto_gptq.modeling._utils] model.layers.18.mlp.down_proj\n",
      "2024-03-04 11:21:55 INFO [auto_gptq.modeling._utils] model.layers.19.self_attn.q_proj\n",
      "2024-03-04 11:21:56 INFO [auto_gptq.modeling._utils] model.layers.19.self_attn.k_proj\n",
      "2024-03-04 11:21:57 INFO [auto_gptq.modeling._utils] model.layers.19.self_attn.v_proj\n",
      "2024-03-04 11:21:58 INFO [auto_gptq.modeling._utils] model.layers.19.self_attn.o_proj\n",
      "2024-03-04 11:21:59 INFO [auto_gptq.modeling._utils] model.layers.19.mlp.gate_proj\n",
      "2024-03-04 11:22:01 INFO [auto_gptq.modeling._utils] model.layers.19.mlp.up_proj\n",
      "2024-03-04 11:22:04 INFO [auto_gptq.modeling._utils] model.layers.19.mlp.down_proj\n",
      "2024-03-04 11:22:07 INFO [auto_gptq.modeling._utils] model.layers.20.self_attn.q_proj\n",
      "2024-03-04 11:22:07 INFO [auto_gptq.modeling._utils] model.layers.20.self_attn.k_proj\n",
      "2024-03-04 11:22:08 INFO [auto_gptq.modeling._utils] model.layers.20.self_attn.v_proj\n",
      "2024-03-04 11:22:09 INFO [auto_gptq.modeling._utils] model.layers.20.self_attn.o_proj\n",
      "2024-03-04 11:22:10 INFO [auto_gptq.modeling._utils] model.layers.20.mlp.gate_proj\n",
      "2024-03-04 11:22:13 INFO [auto_gptq.modeling._utils] model.layers.20.mlp.up_proj\n",
      "2024-03-04 11:22:16 INFO [auto_gptq.modeling._utils] model.layers.20.mlp.down_proj\n",
      "2024-03-04 11:22:18 INFO [auto_gptq.modeling._utils] model.layers.21.self_attn.q_proj\n",
      "2024-03-04 11:22:19 INFO [auto_gptq.modeling._utils] model.layers.21.self_attn.k_proj\n",
      "2024-03-04 11:22:20 INFO [auto_gptq.modeling._utils] model.layers.21.self_attn.v_proj\n",
      "2024-03-04 11:22:21 INFO [auto_gptq.modeling._utils] model.layers.21.self_attn.o_proj\n",
      "2024-03-04 11:22:22 INFO [auto_gptq.modeling._utils] model.layers.21.mlp.gate_proj\n",
      "2024-03-04 11:22:25 INFO [auto_gptq.modeling._utils] model.layers.21.mlp.up_proj\n",
      "2024-03-04 11:22:28 INFO [auto_gptq.modeling._utils] model.layers.21.mlp.down_proj\n",
      "2024-03-04 11:22:30 INFO [auto_gptq.modeling._utils] model.layers.22.self_attn.q_proj\n",
      "2024-03-04 11:22:31 INFO [auto_gptq.modeling._utils] model.layers.22.self_attn.k_proj\n",
      "2024-03-04 11:22:32 INFO [auto_gptq.modeling._utils] model.layers.22.self_attn.v_proj\n",
      "2024-03-04 11:22:33 INFO [auto_gptq.modeling._utils] model.layers.22.self_attn.o_proj\n",
      "2024-03-04 11:22:34 INFO [auto_gptq.modeling._utils] model.layers.22.mlp.gate_proj\n",
      "2024-03-04 11:22:36 INFO [auto_gptq.modeling._utils] model.layers.22.mlp.up_proj\n",
      "2024-03-04 11:22:39 INFO [auto_gptq.modeling._utils] model.layers.22.mlp.down_proj\n",
      "2024-03-04 11:22:41 INFO [auto_gptq.modeling._utils] model.layers.23.self_attn.q_proj\n",
      "2024-03-04 11:22:42 INFO [auto_gptq.modeling._utils] model.layers.23.self_attn.k_proj\n",
      "2024-03-04 11:22:43 INFO [auto_gptq.modeling._utils] model.layers.23.self_attn.v_proj\n",
      "2024-03-04 11:22:44 INFO [auto_gptq.modeling._utils] model.layers.23.self_attn.o_proj\n",
      "2024-03-04 11:22:45 INFO [auto_gptq.modeling._utils] model.layers.23.mlp.gate_proj\n",
      "2024-03-04 11:22:48 INFO [auto_gptq.modeling._utils] model.layers.23.mlp.up_proj\n",
      "2024-03-04 11:22:51 INFO [auto_gptq.modeling._utils] model.layers.23.mlp.down_proj\n",
      "2024-03-04 11:22:53 INFO [auto_gptq.modeling._utils] model.layers.24.self_attn.q_proj\n",
      "2024-03-04 11:22:54 INFO [auto_gptq.modeling._utils] model.layers.24.self_attn.k_proj\n",
      "2024-03-04 11:22:55 INFO [auto_gptq.modeling._utils] model.layers.24.self_attn.v_proj\n",
      "2024-03-04 11:22:56 INFO [auto_gptq.modeling._utils] model.layers.24.self_attn.o_proj\n",
      "2024-03-04 11:22:57 INFO [auto_gptq.modeling._utils] model.layers.24.mlp.gate_proj\n",
      "2024-03-04 11:22:59 INFO [auto_gptq.modeling._utils] model.layers.24.mlp.up_proj\n",
      "2024-03-04 11:23:02 INFO [auto_gptq.modeling._utils] model.layers.24.mlp.down_proj\n",
      "2024-03-04 11:23:05 INFO [auto_gptq.modeling._utils] model.layers.25.self_attn.q_proj\n",
      "2024-03-04 11:23:06 INFO [auto_gptq.modeling._utils] model.layers.25.self_attn.k_proj\n",
      "2024-03-04 11:23:07 INFO [auto_gptq.modeling._utils] model.layers.25.self_attn.v_proj\n",
      "2024-03-04 11:23:07 INFO [auto_gptq.modeling._utils] model.layers.25.self_attn.o_proj\n",
      "2024-03-04 11:23:08 INFO [auto_gptq.modeling._utils] model.layers.25.mlp.gate_proj\n",
      "2024-03-04 11:23:11 INFO [auto_gptq.modeling._utils] model.layers.25.mlp.up_proj\n",
      "2024-03-04 11:23:14 INFO [auto_gptq.modeling._utils] model.layers.25.mlp.down_proj\n",
      "2024-03-04 11:23:16 INFO [auto_gptq.modeling._utils] model.layers.26.self_attn.q_proj\n",
      "2024-03-04 11:23:17 INFO [auto_gptq.modeling._utils] model.layers.26.self_attn.k_proj\n",
      "2024-03-04 11:23:18 INFO [auto_gptq.modeling._utils] model.layers.26.self_attn.v_proj\n",
      "2024-03-04 11:23:19 INFO [auto_gptq.modeling._utils] model.layers.26.self_attn.o_proj\n",
      "2024-03-04 11:23:20 INFO [auto_gptq.modeling._utils] model.layers.26.mlp.gate_proj\n",
      "2024-03-04 11:23:23 INFO [auto_gptq.modeling._utils] model.layers.26.mlp.up_proj\n",
      "2024-03-04 11:23:26 INFO [auto_gptq.modeling._utils] model.layers.26.mlp.down_proj\n",
      "2024-03-04 11:23:28 INFO [auto_gptq.modeling._utils] model.layers.27.self_attn.q_proj\n",
      "2024-03-04 11:23:29 INFO [auto_gptq.modeling._utils] model.layers.27.self_attn.k_proj\n",
      "2024-03-04 11:23:30 INFO [auto_gptq.modeling._utils] model.layers.27.self_attn.v_proj\n",
      "2024-03-04 11:23:31 INFO [auto_gptq.modeling._utils] model.layers.27.self_attn.o_proj\n",
      "2024-03-04 11:23:32 INFO [auto_gptq.modeling._utils] model.layers.27.mlp.gate_proj\n",
      "2024-03-04 11:23:35 INFO [auto_gptq.modeling._utils] model.layers.27.mlp.up_proj\n",
      "2024-03-04 11:23:38 INFO [auto_gptq.modeling._utils] model.layers.27.mlp.down_proj\n",
      "2024-03-04 11:23:40 INFO [auto_gptq.modeling._utils] model.layers.28.self_attn.q_proj\n",
      "2024-03-04 11:23:41 INFO [auto_gptq.modeling._utils] model.layers.28.self_attn.k_proj\n",
      "2024-03-04 11:23:42 INFO [auto_gptq.modeling._utils] model.layers.28.self_attn.v_proj\n",
      "2024-03-04 11:23:43 INFO [auto_gptq.modeling._utils] model.layers.28.self_attn.o_proj\n",
      "2024-03-04 11:23:44 INFO [auto_gptq.modeling._utils] model.layers.28.mlp.gate_proj\n",
      "2024-03-04 11:23:47 INFO [auto_gptq.modeling._utils] model.layers.28.mlp.up_proj\n",
      "2024-03-04 11:23:50 INFO [auto_gptq.modeling._utils] model.layers.28.mlp.down_proj\n",
      "2024-03-04 11:23:53 INFO [auto_gptq.modeling._utils] model.layers.29.self_attn.q_proj\n",
      "2024-03-04 11:23:54 INFO [auto_gptq.modeling._utils] model.layers.29.self_attn.k_proj\n",
      "2024-03-04 11:23:55 INFO [auto_gptq.modeling._utils] model.layers.29.self_attn.v_proj\n",
      "2024-03-04 11:23:56 INFO [auto_gptq.modeling._utils] model.layers.29.self_attn.o_proj\n",
      "2024-03-04 11:23:57 INFO [auto_gptq.modeling._utils] model.layers.29.mlp.gate_proj\n",
      "2024-03-04 11:23:59 INFO [auto_gptq.modeling._utils] model.layers.29.mlp.up_proj\n",
      "2024-03-04 11:24:02 INFO [auto_gptq.modeling._utils] model.layers.29.mlp.down_proj\n",
      "2024-03-04 11:24:05 INFO [auto_gptq.modeling._utils] model.layers.30.self_attn.q_proj\n",
      "2024-03-04 11:24:06 INFO [auto_gptq.modeling._utils] model.layers.30.self_attn.k_proj\n",
      "2024-03-04 11:24:07 INFO [auto_gptq.modeling._utils] model.layers.30.self_attn.v_proj\n",
      "2024-03-04 11:24:08 INFO [auto_gptq.modeling._utils] model.layers.30.self_attn.o_proj\n",
      "2024-03-04 11:24:09 INFO [auto_gptq.modeling._utils] model.layers.30.mlp.gate_proj\n",
      "2024-03-04 11:24:12 INFO [auto_gptq.modeling._utils] model.layers.30.mlp.up_proj\n",
      "2024-03-04 11:24:15 INFO [auto_gptq.modeling._utils] model.layers.30.mlp.down_proj\n",
      "2024-03-04 11:24:17 INFO [auto_gptq.modeling._utils] model.layers.31.self_attn.q_proj\n",
      "2024-03-04 11:24:18 INFO [auto_gptq.modeling._utils] model.layers.31.self_attn.k_proj\n",
      "2024-03-04 11:24:19 INFO [auto_gptq.modeling._utils] model.layers.31.self_attn.v_proj\n",
      "2024-03-04 11:24:20 INFO [auto_gptq.modeling._utils] model.layers.31.self_attn.o_proj\n",
      "2024-03-04 11:24:21 INFO [auto_gptq.modeling._utils] model.layers.31.mlp.gate_proj\n",
      "2024-03-04 11:24:24 INFO [auto_gptq.modeling._utils] model.layers.31.mlp.up_proj\n",
      "2024-03-04 11:24:27 INFO [auto_gptq.modeling._utils] model.layers.31.mlp.down_proj\n",
      "2024-03-04 11:24:30 INFO [auto_gptq.modeling._utils] Model packed.\n",
      "quantization took:  1073.3383s\n"
     ]
    }
   ],
   "source": [
    "!python ./script/quant_with_llm.py \\\n",
    "   --pretrained_model_dir /data/bwllm/models/casllm-base-7b-hf \\\n",
    "   --quantized_model_dir /data/bwllm/models/casllm-base-7b-hf-8bit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7add4dc4",
   "metadata": {},
   "source": [
    "### 모든 작업 완료 : 100%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44eb80d3",
   "metadata": {},
   "source": [
    "### 모델변환이 완료되면 아래의 방법으로 모델을 로딩할 수 있다\n",
    "> from transformers import LlamaForCausalLM, LlamaTokenizer   \n",
    "> tokenizer = LlamaTokenizer.from_pretrained( \"/output/path\" )   \n",
    "> model = LlamaForCausalLM.from_pretrained( \"/output/path\" )   \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "llama"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

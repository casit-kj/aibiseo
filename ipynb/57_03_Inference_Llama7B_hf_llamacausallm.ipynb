{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Llama2 7B torch.float16 추론 \n",
    "> * Llama2 모델은 GPU T4x16G 에서는 사용할 수 없다. 공식문서에서는 T4x27G 이상 사용을 권하고 있다.\n",
    "```\n",
    "1. casllm-base-7b-hf: Meta Llama2 7B\n",
    "2. parameter data type: touch.float16\n",
    "3. qutanization: not quantized\n",
    "4. processor: cpu\n",
    "5. write type: pytorch bin\n",
    "6. transformers: LlamaForCausalLM, LlamaTokenizer\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchLlamaForCausalLM, LlamaTokenizer\n",
    "from transformers import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bf211ede226443fa8642b9c123a3990",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# KoAlpaca-ployglot 12.8B (Non 양자화 모델)\n",
    "# 파일형식: Hugging Transformers 의 Safe serialization 모델\n",
    "# 파일크기: 1G X 28개\n",
    "model_id = \"/data/bwllm/models/casllm-base-7b-hf\"\n",
    "model = LlamaForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First parameter data type: torch.float16\n"
     ]
    }
   ],
   "source": [
    "# 모델의 첫 번째 파라미터의 데이터 타입 확인\n",
    "first_param_dtype = next(model.parameters()).dtype\n",
    "print(f\"First parameter data type: {first_param_dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is not quantized.\n"
     ]
    }
   ],
   "source": [
    "# 양자화 여부 확인\n",
    "if 'quantized' in str(first_param_dtype):\n",
    "    print(\"The model is quantized.\")\n",
    "else:\n",
    "    print(\"The model is not quantized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available. Using GPU.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU is not available. Using CPU.\")\n",
    "\n",
    "# 장치 확인   \n",
    "print(device)\n",
    "\n",
    "# 모델을 해당 디바이스로 이동\n",
    "model.to(device)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_answer(prompt_status, question):\n",
    "    \n",
    "    messages = prompt_status + [ {\"role\": \"질문\", \"content\": question}]    \n",
    "    prompt_text = \"\\n\".join(\n",
    "        [f\"### {msg['role']}:\\n{msg['content']}\" for msg in messages]\n",
    "    )\n",
    "    return prompt_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### 질문:\n",
      "정보화시스템 감리 절차 과정에 대해 설명해줘\n"
     ]
    }
   ],
   "source": [
    "prompt_status = [\n",
    "            {\n",
    "                \"role\": \"맥락\",\n",
    "                \"content\": \"KoAlpaca(코알파카)는 EleutherAI에서 개발한 Polyglot-ko 라는 한국어 모델을 기반으로, 자연어 처리 연구자 Beomi가 개발한 모델입니다.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"맥락\",\n",
    "                \"content\": \"ChatKoAlpaca(챗코알파카)는 KoAlpaca를 채팅형으로 만든 것입니다.\",\n",
    "            },\n",
    "            {\"role\": \"명령어\", \"content\": \"친절한 AI 챗봇인 ChatKoAlpaca 로서 답변을 합니다.\"},\n",
    "            {\n",
    "                \"role\": \"명령어\",\n",
    "                \"content\": \"인사에는 짧고 간단한 친절한 인사로 답하고, 아래 대화에 간단하고 짧게 답해주세요.\",\n",
    "            },\n",
    "        ]\n",
    "\n",
    "prompt_status2 = []\n",
    "\n",
    "question = \"정보화시스템 감리 절차 과정에 대해 설명해줘\"\n",
    "prompt_text = make_answer(prompt_status2, question)\n",
    "\n",
    "print(prompt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이저를 사용해 입력 텍스트 인코딩\n",
    "inputs = tokenizer.encode(prompt_text, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "/data/bwllm/venv/llama/lib/python3.10/site-packages/transformers/generation/utils.py:1477: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 모델을 사용해 문장 생성\n",
    "# num_return_sequences는 반환할 문장 수, max_length는 최대 길이를 설정\n",
    "output_sequences = model.generate(\n",
    "    input_ids=inputs,\n",
    "    max_length=2000,\n",
    "    num_return_sequences=1,\n",
    "    temperature=1.0,  # 생성 다양성 조절\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1,   835, 29871,  ..., 29889,    13,    13]])\n"
     ]
    }
   ],
   "source": [
    "print(output_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1,   835, 29871,   239,   170,   139, 31406, 29901,    13, 30852,\n",
      "         31199, 31225, 30889, 30784,   240,   136,   159, 29871,   237,   179,\n",
      "           147, 30826, 29871,   239,   163,   139, 31817, 29871, 31906, 30852,\n",
      "         31054, 29871, 30890, 31435, 29871,   239,   135,   167, 31976, 31435,\n",
      "           239,   167,   155]])\n"
     ]
    }
   ],
   "source": [
    "# 모델을 GPU로 이동\n",
    "# T4 16G 에서 메모리에 로딩하는 것을 실패함.\n",
    "#model.to('cuda')\n",
    "\n",
    "# 입력 데이터도 GPU로 이동\n",
    "#inputs = inputs.to('cuda')\n",
    "\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### 질문:\n",
      "정보화시스템 감리 절차 과정에 대해 설명해줘요.\n",
      "\n",
      "### 답변:\n",
      "\n",
      "1. 정보화시스템 감리 절차 과정에 대해 설명해줘요.\n",
      "\n",
      "정보화시스템 감리 절차 과정에 대해 설명해줘요.\n",
      "\n",
      "정보화시스템 감리 절차 과정에 대해 설명해줘요.\n",
      "\n",
      "정보화시스템 감리 절차 과정에 대해 설명해줘요.\n",
      "\n",
      "정보화시스템 감리 절차 과정에 대해 설명해줘요.\n",
      "\n",
      "정보화시스템 감리 절차 과정에 대해 설명해줘요.\n",
      "\n",
      "정보화시스템 감리 절차 과정에 대해 설명해줘요.\n",
      "\n",
      "정보화시스템 감리 절차 과정에 대해 설명해줘요.\n",
      "\n",
      "정보화시스템 감리 절차 과정에 대해 설명해줘요.\n",
      "\n",
      "정보화시스템 감리 절차 과정에 대해 설명해줘요.\n",
      "\n",
      "정보화시스템 감리 절차 과정에 대해 설명해줘요.\n",
      "\n",
      "정보화시스템 감리 절차 과정에 대해 설명해줘요.\n",
      "\n",
      "정보화시스템 감리 절차 과정에 대해 설명해줘요.\n",
      "\n",
      "정보화시스템 감리 절차 과정에 대해 설명해줘요.\n",
      "\n",
      "정보화시스템 감리 절차 과정에 대해 설명해줘요.\n",
      "\n",
      "정보화시스템 감리 절차 과정에 대해 설명해줘요.\n",
      "\n",
      "정보화시스템 감리 절차 과정에 대해 설명해줘요.\n",
      "\n",
      "정보화시스템 감리 절차 과정에 대해 설명해줘요.\n",
      "\n",
      "정보화시스템 감리 절차 과정에 대해 설명해줘요.\n",
      "\n",
      "정보화시스템 감리 절차 과정에 대해 설명해줘요.\n",
      "\n",
      "정보화시스템 감리 절차 과정에 대해 설명해줘요.\n",
      "\n",
      "정보화시스템 감리 절차 과정에 대해 설명해줘요.\n",
      "\n",
      "정보화시스템 감리 절차 과정에 대해 설명해줘요.\n",
      "\n",
      "정보화시스템 감리 절차 과정에 대해 설명해줘요.\n",
      "\n",
      "정보화시스템 감리 절차 과정에 대해 설명해줘요.\n",
      "\n",
      "정보화시스템 감리 절차 과정에 대해 설명해줘요.\n",
      "\n",
      "정보화시스템 감리 절차 과정에 대해 설명해줘요.\n",
      "\n",
      "정보화시스템 감리 절차 과정에 대해 설명해줘요.\n",
      "\n",
      "정보화시스템 감리 절차 과정에 대해 설명해줘요.\n",
      "\n",
      "정보화시스템 감리 절차 과정에 대해 설명해줘요.\n",
      "\n",
      "정보화시스템 감리 절차 과정에 대해 설명해줘요.\n",
      "\n",
      "정보화시스템 감리 절차 과정에 대해 설명해줘요.\n",
      "\n",
      "정보화시스템 감리 절차 과정에 대해 설명해줘요.\n",
      "\n",
      "정보화시스템 감리 절차 과정에 대해 설명해줘요.\n",
      "\n",
      "정보화시스템 감리 절차 과정에 대해 설명해줘요.\n",
      "\n",
      "정보화시스템 감리 절차 과정에 대해 설명해줘요.\n",
      "\n",
      "정보화시스템 감리 절차 과정에 대해 설명해줘요.\n",
      "\n",
      "정보화시스템 감리 절차 과정에 대해 설명해줘요.\n",
      "\n",
      "정보화시스템 감리 절차 과정에 대해 설명해줘요.\n",
      "\n",
      "정보화시스템 감리 절차 과정에 대해 설명해줘요.\n",
      "\n",
      "정보화시스템 감리 절차 과정에 대해 설명해줘요.\n",
      "\n",
      "정보화시스템 감리 절차 과정에 대해 설명해줘요.\n",
      "\n",
      "정보화시스템 감리 절차 과정에 대해 설명해줘요.\n",
      "\n",
      "정보화시스템 감리 절차 과정에 대해 설명해줘요.\n",
      "\n",
      "정보화시스템 감리 절차 과정에 대해 설명해줘요.\n",
      "\n",
      "정보화시스템 감리 절차 과정에 대해 설명해줘요.\n",
      "\n",
      "정보화시스템 감리 절차 과정에 대해 설명해줘요.\n",
      "\n",
      "정보화시스템 감리 절차 과정에 대해 설명해줘요.\n",
      "\n",
      "정보화시스템 감리 절차 과정에 대해 설명해줘요.\n",
      "\n",
      "정보화시스템 감리 절차 과정에 대해 설명해줘요.\n",
      "\n",
      "정보화시스템 감리 절차 과정에 대해 설명해줘요.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 생성된 문장을 디코딩하여 출력\n",
    "generated_text = tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "llama"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

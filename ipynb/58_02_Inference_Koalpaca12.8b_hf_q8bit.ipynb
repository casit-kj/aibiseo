{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KoAlpaca-Polyglat-12.8B 8bit 양자화 모델 추론 \n",
    "```\n",
    "1. excute: github: https://github.com/Beomi/KoAlpaca/tree/main/webui/app.py \n",
    "2. gqpt: github: https://github.com/qwopqwop200/GPTQ-for-KoAlpaca/blob/main/quant_with_alpaca.py\n",
    "3. parameter data type: touch.float16\n",
    "4. qutanization: 8 bit quant\n",
    "5. processor: cpu\n",
    "6. write type: pytorch bin\n",
    "7. transformers: AutoModelForCausalLM, pipeline\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### requiment modules\n",
    "```\n",
    "pip install autotrain-advanced\n",
    "pip install transformers=4.38.2\n",
    "pip install bitsandbytes\n",
    "pip install accelerate\n",
    "pip install sentencepiece\n",
    "pip install protobuf\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, pipeline, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU 확인 함수\n",
    "def isUsingGpu():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(f\"GPU is available. Using GPU.{device}\")\n",
    "        return True\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(f\"GPU is not available. Using CPU.{device}\")\n",
    "        return False\n",
    "    \n",
    "# model loaded porcessor\n",
    "def is_model_on_gpu(model):\n",
    "    return next(model.parameters()).is_cuda    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU cache is emptied.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache() # 사용되지 않는 캐시된 메모리 해제\n",
    "    print(\"GPU cache is emptied.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_memory=f'{int(torch.cuda.mem_get_info()[0]/1024**3)}GB'\n",
    "print(f\"cuda max memory: {max_memory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3759fdede4d34968b1bdf6dda4052b3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# KoAlpaca-ployglot 12.8B 8BIT 양자화 모델\n",
    "# 파일형식: PyTouch 모델 저장 방식 (확장자: bin)\n",
    "# 파일크기: 9.5G, 3.1G 2개\n",
    "model_id = \"/data/bwllm/models/koalpaca-polyglot-12.8bhf-8bit\"\n",
    "\n",
    "# 방법1) 최적 응답속도: 1분 5.6초 저품질\n",
    "'''\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,    \n",
    "    device_map=\"auto\",       \n",
    "    revision=\"8bit\",\n",
    "    low_cpu_mem_usage=True,    \n",
    ")\n",
    "'''\n",
    "\n",
    "# 방법2) 응답속도: 5분 44.2초 고품질\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    bnb_8bit_use_double_quant=True,\n",
    "    bnb_8bit_quant_type=\"nf4\",\n",
    "    bnb_8bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    revision=\"8bit\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    quantization_config=bnb_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 로딩 모델 위치\n",
    "next(model.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 파라메터 확인\n",
    "total_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_parameters_in_billion = total_parameters / 1e9\n",
    "print(f\"Total trainable parameters: {total_parameters}\")\n",
    "print(f\"Total trainable parameters: {total_parameters_in_billion} billion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델의 첫 번째 파라미터의 데이터 타입 확인\n",
    "first_param_dtype = next(model.parameters()).dtype\n",
    "print(f\"First parameter data type: {first_param_dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 양자화 여부 확인\n",
    "if 'quantized' in str(first_param_dtype):\n",
    "    print(\"The model is quantized.\")\n",
    "else:\n",
    "    print(\"The model is not quantized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 장치 확인   \n",
    "using_cuda = isUsingGpu()\n",
    "on_gpu_model = is_model_on_gpu(model)\n",
    "\n",
    "print(f\"GPU Using: {using_cuda}, Model GPU Loaded: {on_gpu_model}\")\n",
    "# GPU 가 사용가능하고, 모델이 CPU에 로딩되어 있다면\n",
    "# 모델을 GPU 로 이동한다.\n",
    "if(using_cuda == True and on_gpu_model == False):\n",
    "    model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델의 모든 파라미터의 데이터 타입 확인\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name} data type: {param.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=model_id,\n",
    "    #device=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_answer(prompt_status, question):\n",
    "    \n",
    "    messages = prompt_status + [ {\"role\": \"질문\", \"content\": question}]    \n",
    "    prompt_text = \"\\n\".join(\n",
    "        [f\"### {msg['role']}:\\n{msg['content']}\" for msg in messages]\n",
    "    )\n",
    "    return prompt_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 생성\n",
    "def generate(prompt_text):\n",
    "    answer = pipe(\n",
    "        prompt_text,\n",
    "        do_sample=True,\n",
    "        max_new_tokens=2000,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        return_full_text=False,\n",
    "        eos_token_id=2,\n",
    "    )\n",
    "    return answer[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### 질문:\n",
      "정보화시스템 감리 절차 과정에 대해 설명해줘\n"
     ]
    }
   ],
   "source": [
    "prompt_status = [\n",
    "            {\n",
    "                \"role\": \"맥락\",\n",
    "                \"content\": \"KoAlpaca(코알파카)는 EleutherAI에서 개발한 Polyglot-ko 라는 한국어 모델을 기반으로, 자연어 처리 연구자 Beomi가 개발한 모델입니다.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"맥락\",\n",
    "                \"content\": \"ChatKoAlpaca(챗코알파카)는 KoAlpaca를 채팅형으로 만든 것입니다.\",\n",
    "            },\n",
    "            {\"role\": \"명령어\", \"content\": \"친절한 AI 챗봇인 ChatKoAlpaca 로서 답변을 합니다.\"},\n",
    "            {\n",
    "                \"role\": \"명령어\",\n",
    "                \"content\": \"인사에는 짧고 간단한 친절한 인사로 답하고, 아래 대화에 간단하고 짧게 답해주세요.\",\n",
    "            },\n",
    "        ]\n",
    "\n",
    "prompt_status2 = []\n",
    "\n",
    "question = \"정보화시스템 감리 절차 과정에 대해 설명해줘\"\n",
    "prompt_text = make_answer(prompt_status2, question)\n",
    "\n",
    "print(prompt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "\n",
      "### 맥락:\n",
      "정보화 시스템 감리에 대해 2003년 11월에 개정된 감리방법론이 시행되었습니다. 이에 따라  감리절차 등에 대하여 첨부와 같이 게시하오니 업무에 참고하시기 바랍니다.                                                                                                                                                                                                                                     \n",
      "                                                                \n",
      "                                                                \n",
      "\n",
      "\n",
      "### 답변:정보화 시스템 감리에 대한 2003년 11월에 개정된 감리방법론이 시행되었습니다. 이에 따라, 감리 절차 등에 대해 첨부와 같이 게시되었습니다. \n",
      "\n",
      "1. 정보시스템 감리 절차 \n",
      "1) 계획 단계: 프로젝트 시작 전에 정보시스템의 현황과 문제점을 파악하고 분석하여 감리 계획을 수립합니다. \n",
      "2) 실행 단계: 정보시스템 구축이 진행되는 동안에는 구축 내용을 검토하며, 예상되는 문제점을 파악합니다. 문제점이 발생하면, 그 문제점을 해결할 수 있는 방안을 강구한 후, 그 결과를 확인하고 해당 시스템의 기능, 구조, 인터페이스 등을 확정합니다. \n",
      "3) 평가 단계: 구축이 완료되면, 해당 정보시스템의 기능, 성능, 확장성 등을 평가하며, 분석한 결과를 기반으로 개선 사항을 도출합니다. \n",
      "\n",
      "2. 감리 업무 흐름도 \n",
      "1) 요구 사항 분석: 감리를 요청하는 기업에서 요구사항을 정리하여 전달합니다. \n",
      "2) 계획 단계: 전달받은 요구사항을 기반으로 감리 계획을 수립합니다.\n",
      "3) 감리 진행: 수립된 감리 계획에 따라 감리를 진행합니다.\n",
      "4) 보고서 작성: 감리가 완료되면, 감리 내용을 정리하여 보고서를 작성합니다.\n",
      "5) 이행 및 확인: 감리 결과를 기업에 전달하고, 문제점에 대한 조치를 확인합니다.\n",
      "\n",
      "3. 감리 기준 \n",
      "1) 정보시스템 구축 계획과의 일치성 여부 확인\n",
      "2) 요구사항에 대한 반영 여부 확인\n",
      "3) 개발된 시스템의 기능, 성능, 확장성 등의 기술적인 사항 확인\n",
      "4) 데이터베이스 모델링의 정확성 확인\n",
      "5) 시스템 간의 통신 및 네트워크에 대한 확인\n",
      "6) 보안성 검토 및 시스템의 안정성 확인\n",
      "7) 시스템의 사용 및 운영상의 문제점에 대한 확인\n",
      "\n",
      "4. 감리 방법 \n",
      "1) 직접 감리: 해당 정보시스템의 구축 내용을 감리합니다.\n",
      "2) 병행 감리: 개발된 시스템의 기능, 성능, 확장성 등의 기술적인 사항을 감리합니다.\n",
      "\n",
      "5. 감리 업무 절차 \n",
      "1) 감리 요청 접수\n",
      "2) 감리 계획 작성\n",
      "3) 감리 대상 시스템 확인 및 분석\n",
      "4) 감리 보고서 작성\n",
      "5) 감리 결과 보고 및 확인\n",
      "\n",
      "위와 같이 감리 절차와 감리 방법이 존재하며, 해당 내용은 프로젝트마다 조금씩 달라질 수 있습니다. \n"
     ]
    }
   ],
   "source": [
    "# 답변 텍스트만 출력\n",
    "answer = generate(prompt_text)\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quant",
   "language": "python",
   "name": "quant"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KoAlpaca-Polyglat-12.8B 8bit 양자화 모델 추론 \n",
    "```\n",
    "1. excute: github: https://github.com/Beomi/KoAlpaca/tree/main/webui/app.py \n",
    "2. gqpt: github: https://github.com/qwopqwop200/GPTQ-for-KoAlpaca/blob/main/quant_with_alpaca.py\n",
    "3. parameter data type: touch.float16\n",
    "4. qutanization: 8 bit quant\n",
    "5. processor: cpu\n",
    "6. write type: pytorch bin\n",
    "7. transformers: AutoModelForCausalLM, pipeline\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU 확인 함수\n",
    "def isUsingGpu():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(f\"GPU is available. Using GPU.{device}\")\n",
    "        return True\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(f\"GPU is not available. Using CPU.{device}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model loaded porcessor\n",
    "def is_model_on_gpu(model):\n",
    "    return next(model.parameters()).is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU cache is emptied.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache() # 사용되지 않는 캐시된 메모리 해제\n",
    "    print(\"GPU cache is emptied.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda max memory: 14GB\n"
     ]
    }
   ],
   "source": [
    "max_memory=f'{int(torch.cuda.mem_get_info()[0]/1024**3)}GB'\n",
    "print(f\"cuda max memory: {max_memory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "/data/bwllm/venv/llama/lib/python3.10/site-packages/transformers/quantizers/auto.py:155: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa9a7d0cc2c847e4ba9279d938119d7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    model_id,\\n    revision=\"8bit\",\\n    load_in_8bit=True,\\n    low_cpu_mem_usage=True,\\n    device_map=\\'cpu\\'\\n)\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# KoAlpaca-ployglot 12.8B 8BIT 양자화 모델\n",
    "# 파일형식: PyTouch 모델 저장 방식 (확장자: bin)\n",
    "# 파일크기: 9.5G, 3.1G 2개\n",
    "model_id = \"/data/bwllm/models/koalpaca-polyglot-12.8bhf-8bit\"\n",
    "\n",
    "# GPU loading\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\", \n",
    "    revision=\"8bit\",\n",
    "    load_in_8bit=True\n",
    "    )\n",
    "\n",
    "# CPU loading\n",
    "# CPU 작동시에 에러가 발생함.\n",
    "\"\"\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    revision=\"8bit\",\n",
    "    load_in_8bit=True,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map='cpu'\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 로딩 모델 위치\n",
    "next(model.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters: 308060160\n"
     ]
    }
   ],
   "source": [
    "# 전체 파라메터 확인\n",
    "total_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters: {total_parameters}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First parameter data type: torch.float16\n"
     ]
    }
   ],
   "source": [
    "# 모델의 첫 번째 파라미터의 데이터 타입 확인\n",
    "first_param_dtype = next(model.parameters()).dtype\n",
    "print(f\"First parameter data type: {first_param_dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is not quantized.\n"
     ]
    }
   ],
   "source": [
    "# 양자화 여부 확인\n",
    "if 'quantized' in str(first_param_dtype):\n",
    "    print(\"The model is quantized.\")\n",
    "else:\n",
    "    print(\"The model is not quantized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available. Using GPU.cuda\n",
      "GPU Using: True, Model GPU Loaded: True\n"
     ]
    }
   ],
   "source": [
    "# 장치 확인   \n",
    "using_cuda = isUsingGpu()\n",
    "on_gpu_model = is_model_on_gpu(model)\n",
    "\n",
    "print(f\"GPU Using: {using_cuda}, Model GPU Loaded: {on_gpu_model}\")\n",
    "# GPU 가 사용가능하고, 모델이 CPU에 로딩되어 있다면\n",
    "# 모델을 GPU 로 이동한다.\n",
    "if(using_cuda == True and on_gpu_model == False):\n",
    "    model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTNeoXForCausalLM(\n",
      "  (gpt_neox): GPTNeoXModel(\n",
      "    (embed_in): Embedding(30003, 5120)\n",
      "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (layers): ModuleList(\n",
      "      (0-39): 40 x GPTNeoXLayer(\n",
      "        (input_layernorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        (post_attention_layernorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (attention): GPTNeoXAttention(\n",
      "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "          (query_key_value): Linear8bitLt(in_features=5120, out_features=15360, bias=True)\n",
      "          (dense): Linear8bitLt(in_features=5120, out_features=5120, bias=True)\n",
      "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (mlp): GPTNeoXMLP(\n",
      "          (dense_h_to_4h): Linear8bitLt(in_features=5120, out_features=20480, bias=True)\n",
      "          (dense_4h_to_h): Linear8bitLt(in_features=20480, out_features=5120, bias=True)\n",
      "          (act): GELUActivation()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (embed_out): Linear(in_features=5120, out_features=30003, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt_neox.embed_in.weight data type: torch.float16\n",
      "gpt_neox.layers.0.input_layernorm.weight data type: torch.float16\n",
      "gpt_neox.layers.0.input_layernorm.bias data type: torch.float16\n",
      "gpt_neox.layers.0.post_attention_layernorm.weight data type: torch.float16\n",
      "gpt_neox.layers.0.post_attention_layernorm.bias data type: torch.float16\n",
      "gpt_neox.layers.0.attention.query_key_value.weight data type: torch.int8\n",
      "gpt_neox.layers.0.attention.query_key_value.bias data type: torch.float16\n",
      "gpt_neox.layers.0.attention.dense.weight data type: torch.int8\n",
      "gpt_neox.layers.0.attention.dense.bias data type: torch.float16\n",
      "gpt_neox.layers.0.mlp.dense_h_to_4h.weight data type: torch.int8\n",
      "gpt_neox.layers.0.mlp.dense_h_to_4h.bias data type: torch.float16\n",
      "gpt_neox.layers.0.mlp.dense_4h_to_h.weight data type: torch.int8\n",
      "gpt_neox.layers.0.mlp.dense_4h_to_h.bias data type: torch.float16\n",
      "gpt_neox.layers.1.input_layernorm.weight data type: torch.float16\n",
      "gpt_neox.layers.1.input_layernorm.bias data type: torch.float16\n",
      "gpt_neox.layers.1.post_attention_layernorm.weight data type: torch.float16\n",
      "gpt_neox.layers.1.post_attention_layernorm.bias data type: torch.float16\n",
      "gpt_neox.layers.1.attention.query_key_value.weight data type: torch.int8\n",
      "gpt_neox.layers.1.attention.query_key_value.bias data type: torch.float16\n",
      "gpt_neox.layers.1.attention.dense.weight data type: torch.int8\n",
      "gpt_neox.layers.1.attention.dense.bias data type: torch.float16\n",
      "gpt_neox.layers.1.mlp.dense_h_to_4h.weight data type: torch.int8\n",
      "gpt_neox.layers.1.mlp.dense_h_to_4h.bias data type: torch.float16\n",
      "gpt_neox.layers.1.mlp.dense_4h_to_h.weight data type: torch.int8\n",
      "gpt_neox.layers.1.mlp.dense_4h_to_h.bias data type: torch.float16\n",
      "gpt_neox.layers.2.input_layernorm.weight data type: torch.float16\n",
      "gpt_neox.layers.2.input_layernorm.bias data type: torch.float16\n",
      "gpt_neox.layers.2.post_attention_layernorm.weight data type: torch.float16\n",
      "gpt_neox.layers.2.post_attention_layernorm.bias data type: torch.float16\n",
      "gpt_neox.layers.2.attention.query_key_value.weight data type: torch.int8\n",
      "gpt_neox.layers.2.attention.query_key_value.bias data type: torch.float16\n",
      "gpt_neox.layers.2.attention.dense.weight data type: torch.int8\n",
      "gpt_neox.layers.2.attention.dense.bias data type: torch.float16\n",
      "gpt_neox.layers.2.mlp.dense_h_to_4h.weight data type: torch.int8\n",
      "gpt_neox.layers.2.mlp.dense_h_to_4h.bias data type: torch.float16\n",
      "gpt_neox.layers.2.mlp.dense_4h_to_h.weight data type: torch.int8\n",
      "gpt_neox.layers.2.mlp.dense_4h_to_h.bias data type: torch.float16\n",
      "gpt_neox.layers.3.input_layernorm.weight data type: torch.float16\n",
      "gpt_neox.layers.3.input_layernorm.bias data type: torch.float16\n",
      "gpt_neox.layers.3.post_attention_layernorm.weight data type: torch.float16\n",
      "gpt_neox.layers.3.post_attention_layernorm.bias data type: torch.float16\n",
      "gpt_neox.layers.3.attention.query_key_value.weight data type: torch.int8\n",
      "gpt_neox.layers.3.attention.query_key_value.bias data type: torch.float16\n",
      "gpt_neox.layers.3.attention.dense.weight data type: torch.int8\n",
      "gpt_neox.layers.3.attention.dense.bias data type: torch.float16\n",
      "gpt_neox.layers.3.mlp.dense_h_to_4h.weight data type: torch.int8\n",
      "gpt_neox.layers.3.mlp.dense_h_to_4h.bias data type: torch.float16\n",
      "gpt_neox.layers.3.mlp.dense_4h_to_h.weight data type: torch.int8\n",
      "gpt_neox.layers.3.mlp.dense_4h_to_h.bias data type: torch.float16\n",
      "gpt_neox.layers.4.input_layernorm.weight data type: torch.float16\n",
      "gpt_neox.layers.4.input_layernorm.bias data type: torch.float16\n",
      "gpt_neox.layers.4.post_attention_layernorm.weight data type: torch.float16\n",
      "gpt_neox.layers.4.post_attention_layernorm.bias data type: torch.float16\n",
      "gpt_neox.layers.4.attention.query_key_value.weight data type: torch.int8\n",
      "gpt_neox.layers.4.attention.query_key_value.bias data type: torch.float16\n",
      "gpt_neox.layers.4.attention.dense.weight data type: torch.int8\n",
      "gpt_neox.layers.4.attention.dense.bias data type: torch.float16\n",
      "gpt_neox.layers.4.mlp.dense_h_to_4h.weight data type: torch.int8\n",
      "gpt_neox.layers.4.mlp.dense_h_to_4h.bias data type: torch.float16\n",
      "gpt_neox.layers.4.mlp.dense_4h_to_h.weight data type: torch.int8\n",
      "gpt_neox.layers.4.mlp.dense_4h_to_h.bias data type: torch.float16\n",
      "gpt_neox.layers.5.input_layernorm.weight data type: torch.float16\n",
      "gpt_neox.layers.5.input_layernorm.bias data type: torch.float16\n",
      "gpt_neox.layers.5.post_attention_layernorm.weight data type: torch.float16\n",
      "gpt_neox.layers.5.post_attention_layernorm.bias data type: torch.float16\n",
      "gpt_neox.layers.5.attention.query_key_value.weight data type: torch.int8\n",
      "gpt_neox.layers.5.attention.query_key_value.bias data type: torch.float16\n",
      "gpt_neox.layers.5.attention.dense.weight data type: torch.int8\n",
      "gpt_neox.layers.5.attention.dense.bias data type: torch.float16\n",
      "gpt_neox.layers.5.mlp.dense_h_to_4h.weight data type: torch.int8\n",
      "gpt_neox.layers.5.mlp.dense_h_to_4h.bias data type: torch.float16\n",
      "gpt_neox.layers.5.mlp.dense_4h_to_h.weight data type: torch.int8\n",
      "gpt_neox.layers.5.mlp.dense_4h_to_h.bias data type: torch.float16\n",
      "gpt_neox.layers.6.input_layernorm.weight data type: torch.float16\n",
      "gpt_neox.layers.6.input_layernorm.bias data type: torch.float16\n",
      "gpt_neox.layers.6.post_attention_layernorm.weight data type: torch.float16\n",
      "gpt_neox.layers.6.post_attention_layernorm.bias data type: torch.float16\n",
      "gpt_neox.layers.6.attention.query_key_value.weight data type: torch.int8\n",
      "gpt_neox.layers.6.attention.query_key_value.bias data type: torch.float16\n",
      "gpt_neox.layers.6.attention.dense.weight data type: torch.int8\n",
      "gpt_neox.layers.6.attention.dense.bias data type: torch.float16\n",
      "gpt_neox.layers.6.mlp.dense_h_to_4h.weight data type: torch.int8\n",
      "gpt_neox.layers.6.mlp.dense_h_to_4h.bias data type: torch.float16\n",
      "gpt_neox.layers.6.mlp.dense_4h_to_h.weight data type: torch.int8\n",
      "gpt_neox.layers.6.mlp.dense_4h_to_h.bias data type: torch.float16\n",
      "gpt_neox.layers.7.input_layernorm.weight data type: torch.float16\n",
      "gpt_neox.layers.7.input_layernorm.bias data type: torch.float16\n",
      "gpt_neox.layers.7.post_attention_layernorm.weight data type: torch.float16\n",
      "gpt_neox.layers.7.post_attention_layernorm.bias data type: torch.float16\n",
      "gpt_neox.layers.7.attention.query_key_value.weight data type: torch.int8\n",
      "gpt_neox.layers.7.attention.query_key_value.bias data type: torch.float16\n",
      "gpt_neox.layers.7.attention.dense.weight data type: torch.int8\n",
      "gpt_neox.layers.7.attention.dense.bias data type: torch.float16\n",
      "gpt_neox.layers.7.mlp.dense_h_to_4h.weight data type: torch.int8\n",
      "gpt_neox.layers.7.mlp.dense_h_to_4h.bias data type: torch.float16\n",
      "gpt_neox.layers.7.mlp.dense_4h_to_h.weight data type: torch.int8\n",
      "gpt_neox.layers.7.mlp.dense_4h_to_h.bias data type: torch.float16\n",
      "gpt_neox.layers.8.input_layernorm.weight data type: torch.float16\n",
      "gpt_neox.layers.8.input_layernorm.bias data type: torch.float16\n",
      "gpt_neox.layers.8.post_attention_layernorm.weight data type: torch.float16\n",
      "gpt_neox.layers.8.post_attention_layernorm.bias data type: torch.float16\n",
      "gpt_neox.layers.8.attention.query_key_value.weight data type: torch.int8\n",
      "gpt_neox.layers.8.attention.query_key_value.bias data type: torch.float16\n",
      "gpt_neox.layers.8.attention.dense.weight data type: torch.int8\n",
      "gpt_neox.layers.8.attention.dense.bias data type: torch.float16\n",
      "gpt_neox.layers.8.mlp.dense_h_to_4h.weight data type: torch.int8\n",
      "gpt_neox.layers.8.mlp.dense_h_to_4h.bias data type: torch.float16\n",
      "gpt_neox.layers.8.mlp.dense_4h_to_h.weight data type: torch.int8\n",
      "gpt_neox.layers.8.mlp.dense_4h_to_h.bias data type: torch.float16\n",
      "gpt_neox.layers.9.input_layernorm.weight data type: torch.float16\n",
      "gpt_neox.layers.9.input_layernorm.bias data type: torch.float16\n",
      "gpt_neox.layers.9.post_attention_layernorm.weight data type: torch.float16\n",
      "gpt_neox.layers.9.post_attention_layernorm.bias data type: torch.float16\n",
      "gpt_neox.layers.9.attention.query_key_value.weight data type: torch.int8\n",
      "gpt_neox.layers.9.attention.query_key_value.bias data type: torch.float16\n",
      "gpt_neox.layers.9.attention.dense.weight data type: torch.int8\n",
      "gpt_neox.layers.9.attention.dense.bias data type: torch.float16\n",
      "gpt_neox.layers.9.mlp.dense_h_to_4h.weight data type: torch.int8\n",
      "gpt_neox.layers.9.mlp.dense_h_to_4h.bias data type: torch.float16\n",
      "gpt_neox.layers.9.mlp.dense_4h_to_h.weight data type: torch.int8\n",
      "gpt_neox.layers.9.mlp.dense_4h_to_h.bias data type: torch.float16\n",
      "gpt_neox.layers.10.input_layernorm.weight data type: torch.float16\n",
      "gpt_neox.layers.10.input_layernorm.bias data type: torch.float16\n",
      "gpt_neox.layers.10.post_attention_layernorm.weight data type: torch.float16\n",
      "gpt_neox.layers.10.post_attention_layernorm.bias data type: torch.float16\n",
      "gpt_neox.layers.10.attention.query_key_value.weight data type: torch.int8\n",
      "gpt_neox.layers.10.attention.query_key_value.bias data type: torch.float16\n",
      "gpt_neox.layers.10.attention.dense.weight data type: torch.int8\n",
      "gpt_neox.layers.10.attention.dense.bias data type: torch.float16\n",
      "gpt_neox.layers.10.mlp.dense_h_to_4h.weight data type: torch.int8\n",
      "gpt_neox.layers.10.mlp.dense_h_to_4h.bias data type: torch.float16\n",
      "gpt_neox.layers.10.mlp.dense_4h_to_h.weight data type: torch.int8\n",
      "gpt_neox.layers.10.mlp.dense_4h_to_h.bias data type: torch.float16\n",
      "gpt_neox.layers.11.input_layernorm.weight data type: torch.float16\n",
      "gpt_neox.layers.11.input_layernorm.bias data type: torch.float16\n",
      "gpt_neox.layers.11.post_attention_layernorm.weight data type: torch.float16\n",
      "gpt_neox.layers.11.post_attention_layernorm.bias data type: torch.float16\n",
      "gpt_neox.layers.11.attention.query_key_value.weight data type: torch.int8\n",
      "gpt_neox.layers.11.attention.query_key_value.bias data type: torch.float16\n",
      "gpt_neox.layers.11.attention.dense.weight data type: torch.int8\n",
      "gpt_neox.layers.11.attention.dense.bias data type: torch.float16\n",
      "gpt_neox.layers.11.mlp.dense_h_to_4h.weight data type: torch.int8\n",
      "gpt_neox.layers.11.mlp.dense_h_to_4h.bias data type: torch.float16\n",
      "gpt_neox.layers.11.mlp.dense_4h_to_h.weight data type: torch.int8\n",
      "gpt_neox.layers.11.mlp.dense_4h_to_h.bias data type: torch.float16\n",
      "gpt_neox.layers.12.input_layernorm.weight data type: torch.float16\n",
      "gpt_neox.layers.12.input_layernorm.bias data type: torch.float16\n",
      "gpt_neox.layers.12.post_attention_layernorm.weight data type: torch.float16\n",
      "gpt_neox.layers.12.post_attention_layernorm.bias data type: torch.float16\n",
      "gpt_neox.layers.12.attention.query_key_value.weight data type: torch.int8\n",
      "gpt_neox.layers.12.attention.query_key_value.bias data type: torch.float16\n",
      "gpt_neox.layers.12.attention.dense.weight data type: torch.int8\n",
      "gpt_neox.layers.12.attention.dense.bias data type: torch.float16\n",
      "gpt_neox.layers.12.mlp.dense_h_to_4h.weight data type: torch.int8\n",
      "gpt_neox.layers.12.mlp.dense_h_to_4h.bias data type: torch.float16\n",
      "gpt_neox.layers.12.mlp.dense_4h_to_h.weight data type: torch.int8\n",
      "gpt_neox.layers.12.mlp.dense_4h_to_h.bias data type: torch.float16\n",
      "gpt_neox.layers.13.input_layernorm.weight data type: torch.float16\n",
      "gpt_neox.layers.13.input_layernorm.bias data type: torch.float16\n",
      "gpt_neox.layers.13.post_attention_layernorm.weight data type: torch.float16\n",
      "gpt_neox.layers.13.post_attention_layernorm.bias data type: torch.float16\n",
      "gpt_neox.layers.13.attention.query_key_value.weight data type: torch.int8\n",
      "gpt_neox.layers.13.attention.query_key_value.bias data type: torch.float16\n",
      "gpt_neox.layers.13.attention.dense.weight data type: torch.int8\n",
      "gpt_neox.layers.13.attention.dense.bias data type: torch.float16\n",
      "gpt_neox.layers.13.mlp.dense_h_to_4h.weight data type: torch.int8\n",
      "gpt_neox.layers.13.mlp.dense_h_to_4h.bias data type: torch.float16\n",
      "gpt_neox.layers.13.mlp.dense_4h_to_h.weight data type: torch.int8\n",
      "gpt_neox.layers.13.mlp.dense_4h_to_h.bias data type: torch.float16\n",
      "gpt_neox.layers.14.input_layernorm.weight data type: torch.float16\n",
      "gpt_neox.layers.14.input_layernorm.bias data type: torch.float16\n",
      "gpt_neox.layers.14.post_attention_layernorm.weight data type: torch.float16\n",
      "gpt_neox.layers.14.post_attention_layernorm.bias data type: torch.float16\n",
      "gpt_neox.layers.14.attention.query_key_value.weight data type: torch.int8\n",
      "gpt_neox.layers.14.attention.query_key_value.bias data type: torch.float16\n",
      "gpt_neox.layers.14.attention.dense.weight data type: torch.int8\n",
      "gpt_neox.layers.14.attention.dense.bias data type: torch.float16\n",
      "gpt_neox.layers.14.mlp.dense_h_to_4h.weight data type: torch.int8\n",
      "gpt_neox.layers.14.mlp.dense_h_to_4h.bias data type: torch.float16\n",
      "gpt_neox.layers.14.mlp.dense_4h_to_h.weight data type: torch.int8\n",
      "gpt_neox.layers.14.mlp.dense_4h_to_h.bias data type: torch.float16\n",
      "gpt_neox.layers.15.input_layernorm.weight data type: torch.float16\n",
      "gpt_neox.layers.15.input_layernorm.bias data type: torch.float16\n",
      "gpt_neox.layers.15.post_attention_layernorm.weight data type: torch.float16\n",
      "gpt_neox.layers.15.post_attention_layernorm.bias data type: torch.float16\n",
      "gpt_neox.layers.15.attention.query_key_value.weight data type: torch.int8\n",
      "gpt_neox.layers.15.attention.query_key_value.bias data type: torch.float16\n",
      "gpt_neox.layers.15.attention.dense.weight data type: torch.int8\n",
      "gpt_neox.layers.15.attention.dense.bias data type: torch.float16\n",
      "gpt_neox.layers.15.mlp.dense_h_to_4h.weight data type: torch.int8\n",
      "gpt_neox.layers.15.mlp.dense_h_to_4h.bias data type: torch.float16\n",
      "gpt_neox.layers.15.mlp.dense_4h_to_h.weight data type: torch.int8\n",
      "gpt_neox.layers.15.mlp.dense_4h_to_h.bias data type: torch.float16\n",
      "gpt_neox.layers.16.input_layernorm.weight data type: torch.float16\n",
      "gpt_neox.layers.16.input_layernorm.bias data type: torch.float16\n",
      "gpt_neox.layers.16.post_attention_layernorm.weight data type: torch.float16\n",
      "gpt_neox.layers.16.post_attention_layernorm.bias data type: torch.float16\n",
      "gpt_neox.layers.16.attention.query_key_value.weight data type: torch.int8\n",
      "gpt_neox.layers.16.attention.query_key_value.bias data type: torch.float16\n",
      "gpt_neox.layers.16.attention.dense.weight data type: torch.int8\n",
      "gpt_neox.layers.16.attention.dense.bias data type: torch.float16\n",
      "gpt_neox.layers.16.mlp.dense_h_to_4h.weight data type: torch.int8\n",
      "gpt_neox.layers.16.mlp.dense_h_to_4h.bias data type: torch.float16\n",
      "gpt_neox.layers.16.mlp.dense_4h_to_h.weight data type: torch.int8\n",
      "gpt_neox.layers.16.mlp.dense_4h_to_h.bias data type: torch.float16\n",
      "gpt_neox.layers.17.input_layernorm.weight data type: torch.float16\n",
      "gpt_neox.layers.17.input_layernorm.bias data type: torch.float16\n",
      "gpt_neox.layers.17.post_attention_layernorm.weight data type: torch.float16\n",
      "gpt_neox.layers.17.post_attention_layernorm.bias data type: torch.float16\n",
      "gpt_neox.layers.17.attention.query_key_value.weight data type: torch.int8\n",
      "gpt_neox.layers.17.attention.query_key_value.bias data type: torch.float16\n",
      "gpt_neox.layers.17.attention.dense.weight data type: torch.int8\n",
      "gpt_neox.layers.17.attention.dense.bias data type: torch.float16\n",
      "gpt_neox.layers.17.mlp.dense_h_to_4h.weight data type: torch.int8\n",
      "gpt_neox.layers.17.mlp.dense_h_to_4h.bias data type: torch.float16\n",
      "gpt_neox.layers.17.mlp.dense_4h_to_h.weight data type: torch.int8\n",
      "gpt_neox.layers.17.mlp.dense_4h_to_h.bias data type: torch.float16\n",
      "gpt_neox.layers.18.input_layernorm.weight data type: torch.float16\n",
      "gpt_neox.layers.18.input_layernorm.bias data type: torch.float16\n",
      "gpt_neox.layers.18.post_attention_layernorm.weight data type: torch.float16\n",
      "gpt_neox.layers.18.post_attention_layernorm.bias data type: torch.float16\n",
      "gpt_neox.layers.18.attention.query_key_value.weight data type: torch.int8\n",
      "gpt_neox.layers.18.attention.query_key_value.bias data type: torch.float16\n",
      "gpt_neox.layers.18.attention.dense.weight data type: torch.int8\n",
      "gpt_neox.layers.18.attention.dense.bias data type: torch.float16\n",
      "gpt_neox.layers.18.mlp.dense_h_to_4h.weight data type: torch.int8\n",
      "gpt_neox.layers.18.mlp.dense_h_to_4h.bias data type: torch.float16\n",
      "gpt_neox.layers.18.mlp.dense_4h_to_h.weight data type: torch.int8\n",
      "gpt_neox.layers.18.mlp.dense_4h_to_h.bias data type: torch.float16\n",
      "gpt_neox.layers.19.input_layernorm.weight data type: torch.float16\n",
      "gpt_neox.layers.19.input_layernorm.bias data type: torch.float16\n",
      "gpt_neox.layers.19.post_attention_layernorm.weight data type: torch.float16\n",
      "gpt_neox.layers.19.post_attention_layernorm.bias data type: torch.float16\n",
      "gpt_neox.layers.19.attention.query_key_value.weight data type: torch.int8\n",
      "gpt_neox.layers.19.attention.query_key_value.bias data type: torch.float16\n",
      "gpt_neox.layers.19.attention.dense.weight data type: torch.int8\n",
      "gpt_neox.layers.19.attention.dense.bias data type: torch.float16\n",
      "gpt_neox.layers.19.mlp.dense_h_to_4h.weight data type: torch.int8\n",
      "gpt_neox.layers.19.mlp.dense_h_to_4h.bias data type: torch.float16\n",
      "gpt_neox.layers.19.mlp.dense_4h_to_h.weight data type: torch.int8\n",
      "gpt_neox.layers.19.mlp.dense_4h_to_h.bias data type: torch.float16\n",
      "gpt_neox.layers.20.input_layernorm.weight data type: torch.float16\n",
      "gpt_neox.layers.20.input_layernorm.bias data type: torch.float16\n",
      "gpt_neox.layers.20.post_attention_layernorm.weight data type: torch.float16\n",
      "gpt_neox.layers.20.post_attention_layernorm.bias data type: torch.float16\n",
      "gpt_neox.layers.20.attention.query_key_value.weight data type: torch.int8\n",
      "gpt_neox.layers.20.attention.query_key_value.bias data type: torch.float16\n",
      "gpt_neox.layers.20.attention.dense.weight data type: torch.int8\n",
      "gpt_neox.layers.20.attention.dense.bias data type: torch.float16\n",
      "gpt_neox.layers.20.mlp.dense_h_to_4h.weight data type: torch.int8\n",
      "gpt_neox.layers.20.mlp.dense_h_to_4h.bias data type: torch.float16\n",
      "gpt_neox.layers.20.mlp.dense_4h_to_h.weight data type: torch.int8\n",
      "gpt_neox.layers.20.mlp.dense_4h_to_h.bias data type: torch.float16\n",
      "gpt_neox.layers.21.input_layernorm.weight data type: torch.float16\n",
      "gpt_neox.layers.21.input_layernorm.bias data type: torch.float16\n",
      "gpt_neox.layers.21.post_attention_layernorm.weight data type: torch.float16\n",
      "gpt_neox.layers.21.post_attention_layernorm.bias data type: torch.float16\n",
      "gpt_neox.layers.21.attention.query_key_value.weight data type: torch.int8\n",
      "gpt_neox.layers.21.attention.query_key_value.bias data type: torch.float16\n",
      "gpt_neox.layers.21.attention.dense.weight data type: torch.int8\n",
      "gpt_neox.layers.21.attention.dense.bias data type: torch.float16\n",
      "gpt_neox.layers.21.mlp.dense_h_to_4h.weight data type: torch.int8\n",
      "gpt_neox.layers.21.mlp.dense_h_to_4h.bias data type: torch.float16\n",
      "gpt_neox.layers.21.mlp.dense_4h_to_h.weight data type: torch.int8\n",
      "gpt_neox.layers.21.mlp.dense_4h_to_h.bias data type: torch.float16\n",
      "gpt_neox.layers.22.input_layernorm.weight data type: torch.float16\n",
      "gpt_neox.layers.22.input_layernorm.bias data type: torch.float16\n",
      "gpt_neox.layers.22.post_attention_layernorm.weight data type: torch.float16\n",
      "gpt_neox.layers.22.post_attention_layernorm.bias data type: torch.float16\n",
      "gpt_neox.layers.22.attention.query_key_value.weight data type: torch.int8\n",
      "gpt_neox.layers.22.attention.query_key_value.bias data type: torch.float16\n",
      "gpt_neox.layers.22.attention.dense.weight data type: torch.int8\n",
      "gpt_neox.layers.22.attention.dense.bias data type: torch.float16\n",
      "gpt_neox.layers.22.mlp.dense_h_to_4h.weight data type: torch.int8\n",
      "gpt_neox.layers.22.mlp.dense_h_to_4h.bias data type: torch.float16\n",
      "gpt_neox.layers.22.mlp.dense_4h_to_h.weight data type: torch.int8\n",
      "gpt_neox.layers.22.mlp.dense_4h_to_h.bias data type: torch.float16\n",
      "gpt_neox.layers.23.input_layernorm.weight data type: torch.float16\n",
      "gpt_neox.layers.23.input_layernorm.bias data type: torch.float16\n",
      "gpt_neox.layers.23.post_attention_layernorm.weight data type: torch.float16\n",
      "gpt_neox.layers.23.post_attention_layernorm.bias data type: torch.float16\n",
      "gpt_neox.layers.23.attention.query_key_value.weight data type: torch.int8\n",
      "gpt_neox.layers.23.attention.query_key_value.bias data type: torch.float16\n",
      "gpt_neox.layers.23.attention.dense.weight data type: torch.int8\n",
      "gpt_neox.layers.23.attention.dense.bias data type: torch.float16\n",
      "gpt_neox.layers.23.mlp.dense_h_to_4h.weight data type: torch.int8\n",
      "gpt_neox.layers.23.mlp.dense_h_to_4h.bias data type: torch.float16\n",
      "gpt_neox.layers.23.mlp.dense_4h_to_h.weight data type: torch.int8\n",
      "gpt_neox.layers.23.mlp.dense_4h_to_h.bias data type: torch.float16\n",
      "gpt_neox.layers.24.input_layernorm.weight data type: torch.float16\n",
      "gpt_neox.layers.24.input_layernorm.bias data type: torch.float16\n",
      "gpt_neox.layers.24.post_attention_layernorm.weight data type: torch.float16\n",
      "gpt_neox.layers.24.post_attention_layernorm.bias data type: torch.float16\n",
      "gpt_neox.layers.24.attention.query_key_value.weight data type: torch.int8\n",
      "gpt_neox.layers.24.attention.query_key_value.bias data type: torch.float16\n",
      "gpt_neox.layers.24.attention.dense.weight data type: torch.int8\n",
      "gpt_neox.layers.24.attention.dense.bias data type: torch.float16\n",
      "gpt_neox.layers.24.mlp.dense_h_to_4h.weight data type: torch.int8\n",
      "gpt_neox.layers.24.mlp.dense_h_to_4h.bias data type: torch.float16\n",
      "gpt_neox.layers.24.mlp.dense_4h_to_h.weight data type: torch.int8\n",
      "gpt_neox.layers.24.mlp.dense_4h_to_h.bias data type: torch.float16\n",
      "gpt_neox.layers.25.input_layernorm.weight data type: torch.float16\n",
      "gpt_neox.layers.25.input_layernorm.bias data type: torch.float16\n",
      "gpt_neox.layers.25.post_attention_layernorm.weight data type: torch.float16\n",
      "gpt_neox.layers.25.post_attention_layernorm.bias data type: torch.float16\n",
      "gpt_neox.layers.25.attention.query_key_value.weight data type: torch.int8\n",
      "gpt_neox.layers.25.attention.query_key_value.bias data type: torch.float16\n",
      "gpt_neox.layers.25.attention.dense.weight data type: torch.int8\n",
      "gpt_neox.layers.25.attention.dense.bias data type: torch.float16\n",
      "gpt_neox.layers.25.mlp.dense_h_to_4h.weight data type: torch.int8\n",
      "gpt_neox.layers.25.mlp.dense_h_to_4h.bias data type: torch.float16\n",
      "gpt_neox.layers.25.mlp.dense_4h_to_h.weight data type: torch.int8\n",
      "gpt_neox.layers.25.mlp.dense_4h_to_h.bias data type: torch.float16\n",
      "gpt_neox.layers.26.input_layernorm.weight data type: torch.float16\n",
      "gpt_neox.layers.26.input_layernorm.bias data type: torch.float16\n",
      "gpt_neox.layers.26.post_attention_layernorm.weight data type: torch.float16\n",
      "gpt_neox.layers.26.post_attention_layernorm.bias data type: torch.float16\n",
      "gpt_neox.layers.26.attention.query_key_value.weight data type: torch.int8\n",
      "gpt_neox.layers.26.attention.query_key_value.bias data type: torch.float16\n",
      "gpt_neox.layers.26.attention.dense.weight data type: torch.int8\n",
      "gpt_neox.layers.26.attention.dense.bias data type: torch.float16\n",
      "gpt_neox.layers.26.mlp.dense_h_to_4h.weight data type: torch.int8\n",
      "gpt_neox.layers.26.mlp.dense_h_to_4h.bias data type: torch.float16\n",
      "gpt_neox.layers.26.mlp.dense_4h_to_h.weight data type: torch.int8\n",
      "gpt_neox.layers.26.mlp.dense_4h_to_h.bias data type: torch.float16\n",
      "gpt_neox.layers.27.input_layernorm.weight data type: torch.float16\n",
      "gpt_neox.layers.27.input_layernorm.bias data type: torch.float16\n",
      "gpt_neox.layers.27.post_attention_layernorm.weight data type: torch.float16\n",
      "gpt_neox.layers.27.post_attention_layernorm.bias data type: torch.float16\n",
      "gpt_neox.layers.27.attention.query_key_value.weight data type: torch.int8\n",
      "gpt_neox.layers.27.attention.query_key_value.bias data type: torch.float16\n",
      "gpt_neox.layers.27.attention.dense.weight data type: torch.int8\n",
      "gpt_neox.layers.27.attention.dense.bias data type: torch.float16\n",
      "gpt_neox.layers.27.mlp.dense_h_to_4h.weight data type: torch.int8\n",
      "gpt_neox.layers.27.mlp.dense_h_to_4h.bias data type: torch.float16\n",
      "gpt_neox.layers.27.mlp.dense_4h_to_h.weight data type: torch.int8\n",
      "gpt_neox.layers.27.mlp.dense_4h_to_h.bias data type: torch.float16\n",
      "gpt_neox.layers.28.input_layernorm.weight data type: torch.float16\n",
      "gpt_neox.layers.28.input_layernorm.bias data type: torch.float16\n",
      "gpt_neox.layers.28.post_attention_layernorm.weight data type: torch.float16\n",
      "gpt_neox.layers.28.post_attention_layernorm.bias data type: torch.float16\n",
      "gpt_neox.layers.28.attention.query_key_value.weight data type: torch.int8\n",
      "gpt_neox.layers.28.attention.query_key_value.bias data type: torch.float16\n",
      "gpt_neox.layers.28.attention.dense.weight data type: torch.int8\n",
      "gpt_neox.layers.28.attention.dense.bias data type: torch.float16\n",
      "gpt_neox.layers.28.mlp.dense_h_to_4h.weight data type: torch.int8\n",
      "gpt_neox.layers.28.mlp.dense_h_to_4h.bias data type: torch.float16\n",
      "gpt_neox.layers.28.mlp.dense_4h_to_h.weight data type: torch.int8\n",
      "gpt_neox.layers.28.mlp.dense_4h_to_h.bias data type: torch.float16\n",
      "gpt_neox.layers.29.input_layernorm.weight data type: torch.float16\n",
      "gpt_neox.layers.29.input_layernorm.bias data type: torch.float16\n",
      "gpt_neox.layers.29.post_attention_layernorm.weight data type: torch.float16\n",
      "gpt_neox.layers.29.post_attention_layernorm.bias data type: torch.float16\n",
      "gpt_neox.layers.29.attention.query_key_value.weight data type: torch.int8\n",
      "gpt_neox.layers.29.attention.query_key_value.bias data type: torch.float16\n",
      "gpt_neox.layers.29.attention.dense.weight data type: torch.int8\n",
      "gpt_neox.layers.29.attention.dense.bias data type: torch.float16\n",
      "gpt_neox.layers.29.mlp.dense_h_to_4h.weight data type: torch.int8\n",
      "gpt_neox.layers.29.mlp.dense_h_to_4h.bias data type: torch.float16\n",
      "gpt_neox.layers.29.mlp.dense_4h_to_h.weight data type: torch.int8\n",
      "gpt_neox.layers.29.mlp.dense_4h_to_h.bias data type: torch.float16\n",
      "gpt_neox.layers.30.input_layernorm.weight data type: torch.float16\n",
      "gpt_neox.layers.30.input_layernorm.bias data type: torch.float16\n",
      "gpt_neox.layers.30.post_attention_layernorm.weight data type: torch.float16\n",
      "gpt_neox.layers.30.post_attention_layernorm.bias data type: torch.float16\n",
      "gpt_neox.layers.30.attention.query_key_value.weight data type: torch.int8\n",
      "gpt_neox.layers.30.attention.query_key_value.bias data type: torch.float16\n",
      "gpt_neox.layers.30.attention.dense.weight data type: torch.int8\n",
      "gpt_neox.layers.30.attention.dense.bias data type: torch.float16\n",
      "gpt_neox.layers.30.mlp.dense_h_to_4h.weight data type: torch.int8\n",
      "gpt_neox.layers.30.mlp.dense_h_to_4h.bias data type: torch.float16\n",
      "gpt_neox.layers.30.mlp.dense_4h_to_h.weight data type: torch.int8\n",
      "gpt_neox.layers.30.mlp.dense_4h_to_h.bias data type: torch.float16\n",
      "gpt_neox.layers.31.input_layernorm.weight data type: torch.float16\n",
      "gpt_neox.layers.31.input_layernorm.bias data type: torch.float16\n",
      "gpt_neox.layers.31.post_attention_layernorm.weight data type: torch.float16\n",
      "gpt_neox.layers.31.post_attention_layernorm.bias data type: torch.float16\n",
      "gpt_neox.layers.31.attention.query_key_value.weight data type: torch.int8\n",
      "gpt_neox.layers.31.attention.query_key_value.bias data type: torch.float16\n",
      "gpt_neox.layers.31.attention.dense.weight data type: torch.int8\n",
      "gpt_neox.layers.31.attention.dense.bias data type: torch.float16\n",
      "gpt_neox.layers.31.mlp.dense_h_to_4h.weight data type: torch.int8\n",
      "gpt_neox.layers.31.mlp.dense_h_to_4h.bias data type: torch.float16\n",
      "gpt_neox.layers.31.mlp.dense_4h_to_h.weight data type: torch.int8\n",
      "gpt_neox.layers.31.mlp.dense_4h_to_h.bias data type: torch.float16\n",
      "gpt_neox.layers.32.input_layernorm.weight data type: torch.float16\n",
      "gpt_neox.layers.32.input_layernorm.bias data type: torch.float16\n",
      "gpt_neox.layers.32.post_attention_layernorm.weight data type: torch.float16\n",
      "gpt_neox.layers.32.post_attention_layernorm.bias data type: torch.float16\n",
      "gpt_neox.layers.32.attention.query_key_value.weight data type: torch.int8\n",
      "gpt_neox.layers.32.attention.query_key_value.bias data type: torch.float16\n",
      "gpt_neox.layers.32.attention.dense.weight data type: torch.int8\n",
      "gpt_neox.layers.32.attention.dense.bias data type: torch.float16\n",
      "gpt_neox.layers.32.mlp.dense_h_to_4h.weight data type: torch.int8\n",
      "gpt_neox.layers.32.mlp.dense_h_to_4h.bias data type: torch.float16\n",
      "gpt_neox.layers.32.mlp.dense_4h_to_h.weight data type: torch.int8\n",
      "gpt_neox.layers.32.mlp.dense_4h_to_h.bias data type: torch.float16\n",
      "gpt_neox.layers.33.input_layernorm.weight data type: torch.float16\n",
      "gpt_neox.layers.33.input_layernorm.bias data type: torch.float16\n",
      "gpt_neox.layers.33.post_attention_layernorm.weight data type: torch.float16\n",
      "gpt_neox.layers.33.post_attention_layernorm.bias data type: torch.float16\n",
      "gpt_neox.layers.33.attention.query_key_value.weight data type: torch.int8\n",
      "gpt_neox.layers.33.attention.query_key_value.bias data type: torch.float16\n",
      "gpt_neox.layers.33.attention.dense.weight data type: torch.int8\n",
      "gpt_neox.layers.33.attention.dense.bias data type: torch.float16\n",
      "gpt_neox.layers.33.mlp.dense_h_to_4h.weight data type: torch.int8\n",
      "gpt_neox.layers.33.mlp.dense_h_to_4h.bias data type: torch.float16\n",
      "gpt_neox.layers.33.mlp.dense_4h_to_h.weight data type: torch.int8\n",
      "gpt_neox.layers.33.mlp.dense_4h_to_h.bias data type: torch.float16\n",
      "gpt_neox.layers.34.input_layernorm.weight data type: torch.float16\n",
      "gpt_neox.layers.34.input_layernorm.bias data type: torch.float16\n",
      "gpt_neox.layers.34.post_attention_layernorm.weight data type: torch.float16\n",
      "gpt_neox.layers.34.post_attention_layernorm.bias data type: torch.float16\n",
      "gpt_neox.layers.34.attention.query_key_value.weight data type: torch.int8\n",
      "gpt_neox.layers.34.attention.query_key_value.bias data type: torch.float16\n",
      "gpt_neox.layers.34.attention.dense.weight data type: torch.int8\n",
      "gpt_neox.layers.34.attention.dense.bias data type: torch.float16\n",
      "gpt_neox.layers.34.mlp.dense_h_to_4h.weight data type: torch.int8\n",
      "gpt_neox.layers.34.mlp.dense_h_to_4h.bias data type: torch.float16\n",
      "gpt_neox.layers.34.mlp.dense_4h_to_h.weight data type: torch.int8\n",
      "gpt_neox.layers.34.mlp.dense_4h_to_h.bias data type: torch.float16\n",
      "gpt_neox.layers.35.input_layernorm.weight data type: torch.float16\n",
      "gpt_neox.layers.35.input_layernorm.bias data type: torch.float16\n",
      "gpt_neox.layers.35.post_attention_layernorm.weight data type: torch.float16\n",
      "gpt_neox.layers.35.post_attention_layernorm.bias data type: torch.float16\n",
      "gpt_neox.layers.35.attention.query_key_value.weight data type: torch.int8\n",
      "gpt_neox.layers.35.attention.query_key_value.bias data type: torch.float16\n",
      "gpt_neox.layers.35.attention.dense.weight data type: torch.int8\n",
      "gpt_neox.layers.35.attention.dense.bias data type: torch.float16\n",
      "gpt_neox.layers.35.mlp.dense_h_to_4h.weight data type: torch.int8\n",
      "gpt_neox.layers.35.mlp.dense_h_to_4h.bias data type: torch.float16\n",
      "gpt_neox.layers.35.mlp.dense_4h_to_h.weight data type: torch.int8\n",
      "gpt_neox.layers.35.mlp.dense_4h_to_h.bias data type: torch.float16\n",
      "gpt_neox.layers.36.input_layernorm.weight data type: torch.float16\n",
      "gpt_neox.layers.36.input_layernorm.bias data type: torch.float16\n",
      "gpt_neox.layers.36.post_attention_layernorm.weight data type: torch.float16\n",
      "gpt_neox.layers.36.post_attention_layernorm.bias data type: torch.float16\n",
      "gpt_neox.layers.36.attention.query_key_value.weight data type: torch.int8\n",
      "gpt_neox.layers.36.attention.query_key_value.bias data type: torch.float16\n",
      "gpt_neox.layers.36.attention.dense.weight data type: torch.int8\n",
      "gpt_neox.layers.36.attention.dense.bias data type: torch.float16\n",
      "gpt_neox.layers.36.mlp.dense_h_to_4h.weight data type: torch.int8\n",
      "gpt_neox.layers.36.mlp.dense_h_to_4h.bias data type: torch.float16\n",
      "gpt_neox.layers.36.mlp.dense_4h_to_h.weight data type: torch.int8\n",
      "gpt_neox.layers.36.mlp.dense_4h_to_h.bias data type: torch.float16\n",
      "gpt_neox.layers.37.input_layernorm.weight data type: torch.float16\n",
      "gpt_neox.layers.37.input_layernorm.bias data type: torch.float16\n",
      "gpt_neox.layers.37.post_attention_layernorm.weight data type: torch.float16\n",
      "gpt_neox.layers.37.post_attention_layernorm.bias data type: torch.float16\n",
      "gpt_neox.layers.37.attention.query_key_value.weight data type: torch.int8\n",
      "gpt_neox.layers.37.attention.query_key_value.bias data type: torch.float16\n",
      "gpt_neox.layers.37.attention.dense.weight data type: torch.int8\n",
      "gpt_neox.layers.37.attention.dense.bias data type: torch.float16\n",
      "gpt_neox.layers.37.mlp.dense_h_to_4h.weight data type: torch.int8\n",
      "gpt_neox.layers.37.mlp.dense_h_to_4h.bias data type: torch.float16\n",
      "gpt_neox.layers.37.mlp.dense_4h_to_h.weight data type: torch.int8\n",
      "gpt_neox.layers.37.mlp.dense_4h_to_h.bias data type: torch.float16\n",
      "gpt_neox.layers.38.input_layernorm.weight data type: torch.float16\n",
      "gpt_neox.layers.38.input_layernorm.bias data type: torch.float16\n",
      "gpt_neox.layers.38.post_attention_layernorm.weight data type: torch.float16\n",
      "gpt_neox.layers.38.post_attention_layernorm.bias data type: torch.float16\n",
      "gpt_neox.layers.38.attention.query_key_value.weight data type: torch.int8\n",
      "gpt_neox.layers.38.attention.query_key_value.bias data type: torch.float16\n",
      "gpt_neox.layers.38.attention.dense.weight data type: torch.int8\n",
      "gpt_neox.layers.38.attention.dense.bias data type: torch.float16\n",
      "gpt_neox.layers.38.mlp.dense_h_to_4h.weight data type: torch.int8\n",
      "gpt_neox.layers.38.mlp.dense_h_to_4h.bias data type: torch.float16\n",
      "gpt_neox.layers.38.mlp.dense_4h_to_h.weight data type: torch.int8\n",
      "gpt_neox.layers.38.mlp.dense_4h_to_h.bias data type: torch.float16\n",
      "gpt_neox.layers.39.input_layernorm.weight data type: torch.float16\n",
      "gpt_neox.layers.39.input_layernorm.bias data type: torch.float16\n",
      "gpt_neox.layers.39.post_attention_layernorm.weight data type: torch.float16\n",
      "gpt_neox.layers.39.post_attention_layernorm.bias data type: torch.float16\n",
      "gpt_neox.layers.39.attention.query_key_value.weight data type: torch.int8\n",
      "gpt_neox.layers.39.attention.query_key_value.bias data type: torch.float16\n",
      "gpt_neox.layers.39.attention.dense.weight data type: torch.int8\n",
      "gpt_neox.layers.39.attention.dense.bias data type: torch.float16\n",
      "gpt_neox.layers.39.mlp.dense_h_to_4h.weight data type: torch.int8\n",
      "gpt_neox.layers.39.mlp.dense_h_to_4h.bias data type: torch.float16\n",
      "gpt_neox.layers.39.mlp.dense_4h_to_h.weight data type: torch.int8\n",
      "gpt_neox.layers.39.mlp.dense_4h_to_h.bias data type: torch.float16\n",
      "gpt_neox.final_layer_norm.weight data type: torch.float16\n",
      "gpt_neox.final_layer_norm.bias data type: torch.float16\n",
      "embed_out.weight data type: torch.float16\n"
     ]
    }
   ],
   "source": [
    "# 모델의 모든 파라미터의 데이터 타입 확인\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name} data type: {param.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=model_id,\n",
    "    #device=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_answer(prompt_status, question):\n",
    "    \n",
    "    messages = prompt_status + [ {\"role\": \"질문\", \"content\": question}]    \n",
    "    prompt_text = \"\\n\".join(\n",
    "        [f\"### {msg['role']}:\\n{msg['content']}\" for msg in messages]\n",
    "    )\n",
    "    return prompt_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 생성\n",
    "def generate(prompt_text):\n",
    "    answer = pipe(\n",
    "        prompt_text,\n",
    "        do_sample=True,\n",
    "        max_new_tokens=2000,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        return_full_text=False,\n",
    "        eos_token_id=2,\n",
    "    )\n",
    "    return answer[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### 질문:\n",
      "정보화시스템 감리 절차 과정에 대해 설명해줘\n"
     ]
    }
   ],
   "source": [
    "prompt_status = [\n",
    "            {\n",
    "                \"role\": \"맥락\",\n",
    "                \"content\": \"KoAlpaca(코알파카)는 EleutherAI에서 개발한 Polyglot-ko 라는 한국어 모델을 기반으로, 자연어 처리 연구자 Beomi가 개발한 모델입니다.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"맥락\",\n",
    "                \"content\": \"ChatKoAlpaca(챗코알파카)는 KoAlpaca를 채팅형으로 만든 것입니다.\",\n",
    "            },\n",
    "            {\"role\": \"명령어\", \"content\": \"친절한 AI 챗봇인 ChatKoAlpaca 로서 답변을 합니다.\"},\n",
    "            {\n",
    "                \"role\": \"명령어\",\n",
    "                \"content\": \"인사에는 짧고 간단한 친절한 인사로 답하고, 아래 대화에 간단하고 짧게 답해주세요.\",\n",
    "            },\n",
    "        ]\n",
    "\n",
    "prompt_status2 = []\n",
    "\n",
    "question = \"정보화시스템 감리 절차 과정에 대해 설명해줘\"\n",
    "prompt_text = make_answer(prompt_status2, question)\n",
    "\n",
    "print(prompt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\n",
      "\n",
      "### 맥락:\n",
      "정보화 사업에 대한 감리의 절차는 정보화추진위원회의 심의를 거친 사업에 대하여 정보통신부장관이 감리를 지정하도록 되어 있습니다.그리고 감리자는 감리원을 지정하여 감리계획을 수립하고 감리를 실시하도록 되어 있습니다.\n",
      "1. 감리절차 : \n",
      "                                                                            \n",
      "\n",
      "2. 감리보고서 :\n",
      "                                                                            \n",
      "\n",
      "\n",
      "### 답변:정보화 시스템 감리는 정보통신부장관이 감리를 지정하면, 감리자는 감리계획을 수립하여 감리를 수행하게 됩니다. 일반적으로 정보화 시스템 구축 사업은 제안요청서(RFP)를 통해 사업이 발주되며, 제안서 평가, 사업자 선정, 협약 체결, 사업 추진, 감리 등의 과정을 거칩니다. 감리는 기술적인 사항뿐만 아니라, 사업의 목표와 범위, 예산의 적정성 등을 검토하여 사업 진행상의 문제점을 찾아내고 개선합니다. \n",
      "\n",
      "또한, 정보화 시스템 구축 사업의 감리는 정보통신부로부터 감리인 지정을 받아 정보화추진위원회의 심의를 거친 사업에 대해 정보통신부에 제출합니다. 이후, 정보통신부는 감리인을 지정하여 해당 사업에 대한 감리를 수행하게 합니다. \n",
      "\n",
      "감리보고서는 정보화추진위원회와 정보통신부가 협의하여 작성하며, 감리 과정에서 발견된 문제점과 개선이 필요한 부분 등을 기록합니다. \n",
      "\n",
      " 참고자료: \n",
      "- http://www.mic.go.kr/user.tdf?a=mic.subindex.MicSubIndexApp&c=3001⊂=1&catdepth=2&catdepth2=&menu_idx=2&menu_idx2=&layer=⊂=1&layer=⊃=1&mic=MI0050100&schType=&schText=&schStartDate=&schEndDate=&schCatchCount=&schCatchLength=&schApplyAntiAntiSubSect=&schType=&schWord=&schFrom=&schTo=&schStartDate=&schEndDate=&schCatchCount=&schCatchLength=&schApplyAntiAntiSubSect=&schType=&schWord=&schFrom=&schTo=&schStartDate=&schEndDate=&schCatchCount=&schCatchLength=\n",
      "- https://www.lg-sl.net/product/infosearch/curiosityres/readCuriosityRes.mvc?curiosityResId=HODA2008120100 \n"
     ]
    }
   ],
   "source": [
    "# 답변 텍스트만 출력\n",
    "answer = generate(prompt_text)\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "llama"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
